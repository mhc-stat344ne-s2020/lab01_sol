{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_20200124.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ht1ZhJT4l7",
        "colab_type": "text"
      },
      "source": [
        "# Goals\n",
        "\n",
        "The purpose of this lab is to give you a chance to practice using NumPy for manipulating arrays and making plots with matplotlib.\n",
        "\n",
        "Along the way, you'll see some of the ideas about neural networks reinforced.\n",
        "\n",
        "# Example\n",
        "\n",
        "We will work with an example of using logistic regression to predict whether someone will develop coronary heart disease (chd, our response variable) based on characteristics like their age, blood pressure, ldl cholesterol level, presence/absence of CHD in their family history, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnDTxFk02jY9",
        "colab_type": "text"
      },
      "source": [
        "# Importing Python Modules\n",
        "\n",
        "These modules extend the base functionality in Python for working with data, making plots, and fitting neural network models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvAMs0xNRa3V",
        "colab_type": "code",
        "outputId": "d163fdd1-b0ba-4fe7-c3da-72190e6e1e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "np.random.seed(9533)\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import initializers\n",
        "from keras import optimizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xkCUZpp27N4",
        "colab_type": "text"
      },
      "source": [
        "# Reading in the data, initial set up\n",
        "For now, we want to concentrate on lower level calculations using NumPy; we'll talk more about using Pandas later.  That means I will do some preliminary data manipulation for us so that we can get on to today's topics, but we'll come back and talk about this kind of thing in more detail later :)  \n",
        "\n",
        "The code below reads in the data and takes a quick look at the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JE7IYojTo1E",
        "colab_type": "code",
        "outputId": "68fd743c-532c-4469-a8f1-861ef4473c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "saheart = pd.read_csv(\"http://www.evanlray.com/data/ESL/SAheart.data.txt\").iloc[:, 1:11]\n",
        "print(saheart.head())\n",
        "print(\"shape (rows, columns) = \" + str(saheart.shape))\n",
        "print(\"column names are: \" + str(saheart.columns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
            "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
            "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
            "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
            "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
            "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1\n",
            "shape (rows, columns) = (462, 10)\n",
            "column names are: Index(['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity',\n",
            "       'alcohol', 'age', 'chd'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWSmq7qvEVjz",
        "colab_type": "text"
      },
      "source": [
        "We need to convert famhist to a one-hot encoding, otherwise known as indicator or dummy variable.  The Pandas function `get_dummies` can do this for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8-xYw63BGSf",
        "colab_type": "code",
        "outputId": "2c371cf3-7b81-4575-d13a-97eaf86ee1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "saheart = pd.get_dummies(saheart, drop_first = True)\n",
        "print(saheart.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sbp  tobacco   ldl  adiposity  ...  alcohol  age  chd  famhist_Present\n",
            "0  160    12.00  5.73      23.11  ...    97.20   52    1                1\n",
            "1  144     0.01  4.41      28.61  ...     2.06   63    1                0\n",
            "2  118     0.08  3.48      32.28  ...     3.81   46    0                1\n",
            "3  170     7.50  6.41      38.03  ...    24.26   58    1                1\n",
            "4  134    13.60  3.50      27.78  ...    57.34   49    1                1\n",
            "\n",
            "[5 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP7OvQ3RFC_u",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to extract the data to NumPy arrays using the `to_numpy` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV-EZNCfFA-v",
        "colab_type": "code",
        "outputId": "3af83ca7-d269-4fa9-ced6-d97896b59962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# X will include all columns other than the response, chd;\n",
        "# we drop that column and convert the others to numpy\n",
        "X = saheart.drop('chd', axis = 1).to_numpy()\n",
        "print(X[0:5, ])\n",
        "\n",
        "# y will include only the chd column\n",
        "y = saheart['chd'].to_numpy()\n",
        "y = y.reshape((y.shape[0], 1))\n",
        "print(y[0:5, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.600e+02 1.200e+01 5.730e+00 2.311e+01 4.900e+01 2.530e+01 9.720e+01\n",
            "  5.200e+01 1.000e+00]\n",
            " [1.440e+02 1.000e-02 4.410e+00 2.861e+01 5.500e+01 2.887e+01 2.060e+00\n",
            "  6.300e+01 0.000e+00]\n",
            " [1.180e+02 8.000e-02 3.480e+00 3.228e+01 5.200e+01 2.914e+01 3.810e+00\n",
            "  4.600e+01 1.000e+00]\n",
            " [1.700e+02 7.500e+00 6.410e+00 3.803e+01 5.100e+01 3.199e+01 2.426e+01\n",
            "  5.800e+01 1.000e+00]\n",
            " [1.340e+02 1.360e+01 3.500e+00 2.778e+01 6.000e+01 2.599e+01 5.734e+01\n",
            "  4.900e+01 1.000e+00]]\n",
            "[[1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exwJ8sV4IYQ7",
        "colab_type": "text"
      },
      "source": [
        "I will do one more step for you: a split of the data into train, validation, and test sets.  As we saw last class, model performance on the data we actually use to fit the model tends to be a little better than model performance on new data.  Basically, the model fitting process tunes the model as well as possible to the training data -- but if we look at new data from the same process we may see slightly different patterns.  We are generally interested in seeing how the model does on new data.\n",
        "\n",
        "We will use the train, validation, and test sets as follows:\n",
        " * **train**: used to actually estimate model parameters like $b$ and $w$.\n",
        " * **validation**: used to check in on how model estimation is going and possibly compare a few different candidate models\n",
        " * **test**: once we have selected our final model, we use the test data to see how well it does.\n",
        "\n",
        "Why do we need a test set in addition to a validation set?  When exploring neural networks, we may try a **lot** of different models.  That means that even though the validation set is not directly used to estimate the model parameters, we may end up choosing a model that is specifically tuned to the validation set.  We need to have a test set that was never used for any purpose at all during selecting and estimating a model, to get an honest sense of how well the model does when making predictions for data it has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ZzhDdJY8oT",
        "colab_type": "code",
        "outputId": "8f18409f-5b5a-4af1-f5d0-46cf926db7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(X_train[0:5, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(295, 9)\n",
            "(93, 9)\n",
            "(74, 9)\n",
            "[[160.     4.2    6.76  37.99  61.    32.91   3.09  54.     1.  ]\n",
            " [132.     7.28   3.52  12.33  60.    19.48   2.06  56.     0.  ]\n",
            " [142.     0.     3.54  16.64  58.    25.97   8.36  27.     0.  ]\n",
            " [120.     0.     3.98  13.19  47.    21.89   0.    16.     1.  ]\n",
            " [154.     0.31   2.33  16.48  33.    24.    11.83  17.     0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q6KUj_ANPJp",
        "colab_type": "text"
      },
      "source": [
        "# More data preprocessing\n",
        "\n",
        "### 1. Normalize Input Variables\n",
        "For reasons that we will discuss later, it's critical to the performance of neural networks to \"normalize\" the explanatory (input) variables.  Confusingly, in this context this does **not** mean we want them to follow a normal distribution.  What it means is that they should be standardized to have mean 0 and standard deviation 1.  In statistical terms, basically we want to give the neural network the $z$-scores of our explanatory variables rather than the original explanatory variables.\n",
        "\n",
        "To do this, you will need four lines of code that (1) calculate the column means of X_train, (2) subtract the column means from each column of X_train, (3) calculate the column standard deviations of X_train, and finally (4) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "909reb91_-oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_mean = np.mean(X_train, axis = 0) # add a call to np.mean here.  what will you use for the axis?\n",
        "X_train = X_train - X_train_mean # add code here to subtract the column means from X_train.  How will broadcasting work?\n",
        "X_train_std = np.std(X_train, axis = 0) # add a call to np.std here.  what will you use for the axis?\n",
        "X_train = X_train / X_train_std # add code here to divide X_train by the column standard deviations.  How will broadcasting work?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL2txcKZSCbl",
        "colab_type": "text"
      },
      "source": [
        "Of course, in order to apply our model to make predictions for the validation sets and test sets you will need to apply the same normalization process to X_val and X_test.  Note that we want to use exactly the same normalization for all three; you shouldn't calculate new column means, but subtract X_train_mean from X_val and X_test, and then divide X_val and X_test by X_train_std."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbaQxAtKSs0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize X_val, but using X_train_mean and X_train_std to do the normalization\n",
        "X_val = X_val - X_train_mean  # add code here to subtract the column means from X_val\n",
        "X_val = X_val / X_train_mean  # add code here to divide X_val by the column standard deviations\n",
        "\n",
        "# normalize X_test, but using X_train_mean and X_train_std to do the normalization\n",
        "X_test = X_test - X_train_mean  # add code here to subtract the column means from X_test\n",
        "X_test = X_test / X_train_mean  # add code here to divide X_test by the column standard deviations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwIfG9GFPuEc",
        "colab_type": "text"
      },
      "source": [
        "# Fitting a logistic regression model\n",
        "\n",
        "Again, we're focusing on NumPy in this lab, so I'll provide code to define and fit the model with Keras -- but you would benefit from reading through this code just so that it looks more familiar later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUOa2SbSXHs2",
        "colab_type": "code",
        "outputId": "a72f5214-fa73-41b3-fdbc-fe93cf537a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define a logistic regression model: one layer, with a sigmoid activation and 2 inputs\n",
        "logistic_model = models.Sequential()\n",
        "logistic_model.add(layers.Dense(\n",
        "    1,\n",
        "    activation = 'sigmoid',\n",
        "    input_shape = (9,)))\n",
        "\n",
        "# compile the model using stochastic gradient descent for optimization,\n",
        "# binary cross-entropy loss, and measuring performance by classification accuracy\n",
        "#sgd = optimizers.SGD(lr=0.5, momentum=0.1, nesterov=True)\n",
        "logistic_model.compile(\n",
        "    optimizer = 'sgd',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "# Estimate the model parameters\n",
        "history = logistic_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_val, y_val),\n",
        "    epochs = 500, batch_size = X_train.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 295 samples, validate on 74 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "295/295 [==============================] - 10s 33ms/step - loss: 0.8290 - acc: 0.4610 - val_loss: 0.8412 - val_acc: 0.5135\n",
            "Epoch 2/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.8270 - acc: 0.4678 - val_loss: 0.8400 - val_acc: 0.5135\n",
            "Epoch 3/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.8250 - acc: 0.4678 - val_loss: 0.8389 - val_acc: 0.5135\n",
            "Epoch 4/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.8230 - acc: 0.4678 - val_loss: 0.8377 - val_acc: 0.5135\n",
            "Epoch 5/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8211 - acc: 0.4780 - val_loss: 0.8366 - val_acc: 0.5270\n",
            "Epoch 6/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.8192 - acc: 0.4780 - val_loss: 0.8354 - val_acc: 0.5405\n",
            "Epoch 7/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.8172 - acc: 0.4746 - val_loss: 0.8343 - val_acc: 0.5405\n",
            "Epoch 8/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8154 - acc: 0.4746 - val_loss: 0.8331 - val_acc: 0.5405\n",
            "Epoch 9/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.8135 - acc: 0.4746 - val_loss: 0.8320 - val_acc: 0.5405\n",
            "Epoch 10/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.8116 - acc: 0.4746 - val_loss: 0.8309 - val_acc: 0.5405\n",
            "Epoch 11/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.8098 - acc: 0.4780 - val_loss: 0.8298 - val_acc: 0.5405\n",
            "Epoch 12/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8080 - acc: 0.4814 - val_loss: 0.8286 - val_acc: 0.5405\n",
            "Epoch 13/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.8062 - acc: 0.4881 - val_loss: 0.8275 - val_acc: 0.5405\n",
            "Epoch 14/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.8044 - acc: 0.4881 - val_loss: 0.8264 - val_acc: 0.5405\n",
            "Epoch 15/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.8026 - acc: 0.4915 - val_loss: 0.8254 - val_acc: 0.5405\n",
            "Epoch 16/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.8009 - acc: 0.4915 - val_loss: 0.8243 - val_acc: 0.5405\n",
            "Epoch 17/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7991 - acc: 0.4915 - val_loss: 0.8232 - val_acc: 0.5405\n",
            "Epoch 18/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7974 - acc: 0.4949 - val_loss: 0.8221 - val_acc: 0.5405\n",
            "Epoch 19/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7957 - acc: 0.4949 - val_loss: 0.8210 - val_acc: 0.5405\n",
            "Epoch 20/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7940 - acc: 0.4949 - val_loss: 0.8200 - val_acc: 0.5405\n",
            "Epoch 21/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.7923 - acc: 0.4983 - val_loss: 0.8189 - val_acc: 0.5405\n",
            "Epoch 22/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7907 - acc: 0.4983 - val_loss: 0.8179 - val_acc: 0.5405\n",
            "Epoch 23/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7890 - acc: 0.4983 - val_loss: 0.8168 - val_acc: 0.5405\n",
            "Epoch 24/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7874 - acc: 0.4983 - val_loss: 0.8158 - val_acc: 0.5405\n",
            "Epoch 25/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7858 - acc: 0.4983 - val_loss: 0.8148 - val_acc: 0.5405\n",
            "Epoch 26/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.7842 - acc: 0.5017 - val_loss: 0.8137 - val_acc: 0.5405\n",
            "Epoch 27/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7826 - acc: 0.5085 - val_loss: 0.8127 - val_acc: 0.5405\n",
            "Epoch 28/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7811 - acc: 0.5051 - val_loss: 0.8117 - val_acc: 0.5405\n",
            "Epoch 29/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.7795 - acc: 0.5051 - val_loss: 0.8107 - val_acc: 0.5405\n",
            "Epoch 30/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7780 - acc: 0.5085 - val_loss: 0.8097 - val_acc: 0.5405\n",
            "Epoch 31/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7764 - acc: 0.5051 - val_loss: 0.8087 - val_acc: 0.5405\n",
            "Epoch 32/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7749 - acc: 0.5051 - val_loss: 0.8077 - val_acc: 0.5405\n",
            "Epoch 33/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.7734 - acc: 0.5051 - val_loss: 0.8067 - val_acc: 0.5405\n",
            "Epoch 34/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7719 - acc: 0.5051 - val_loss: 0.8057 - val_acc: 0.5405\n",
            "Epoch 35/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7705 - acc: 0.5051 - val_loss: 0.8047 - val_acc: 0.5405\n",
            "Epoch 36/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.7690 - acc: 0.5085 - val_loss: 0.8038 - val_acc: 0.5405\n",
            "Epoch 37/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7676 - acc: 0.5119 - val_loss: 0.8028 - val_acc: 0.5405\n",
            "Epoch 38/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7661 - acc: 0.5119 - val_loss: 0.8018 - val_acc: 0.5541\n",
            "Epoch 39/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7647 - acc: 0.5119 - val_loss: 0.8009 - val_acc: 0.5541\n",
            "Epoch 40/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7633 - acc: 0.5153 - val_loss: 0.7999 - val_acc: 0.5541\n",
            "Epoch 41/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7619 - acc: 0.5186 - val_loss: 0.7990 - val_acc: 0.5541\n",
            "Epoch 42/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7605 - acc: 0.5220 - val_loss: 0.7980 - val_acc: 0.5541\n",
            "Epoch 43/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7592 - acc: 0.5220 - val_loss: 0.7971 - val_acc: 0.5541\n",
            "Epoch 44/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7578 - acc: 0.5186 - val_loss: 0.7961 - val_acc: 0.5541\n",
            "Epoch 45/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7565 - acc: 0.5254 - val_loss: 0.7952 - val_acc: 0.5541\n",
            "Epoch 46/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.7552 - acc: 0.5254 - val_loss: 0.7943 - val_acc: 0.5676\n",
            "Epoch 47/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7538 - acc: 0.5254 - val_loss: 0.7934 - val_acc: 0.5676\n",
            "Epoch 48/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7525 - acc: 0.5322 - val_loss: 0.7925 - val_acc: 0.5811\n",
            "Epoch 49/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.7512 - acc: 0.5458 - val_loss: 0.7916 - val_acc: 0.5811\n",
            "Epoch 50/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7500 - acc: 0.5458 - val_loss: 0.7907 - val_acc: 0.5811\n",
            "Epoch 51/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7487 - acc: 0.5458 - val_loss: 0.7898 - val_acc: 0.5811\n",
            "Epoch 52/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.7474 - acc: 0.5458 - val_loss: 0.7889 - val_acc: 0.5811\n",
            "Epoch 53/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7462 - acc: 0.5458 - val_loss: 0.7880 - val_acc: 0.5811\n",
            "Epoch 54/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.7449 - acc: 0.5492 - val_loss: 0.7871 - val_acc: 0.5811\n",
            "Epoch 55/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.7437 - acc: 0.5525 - val_loss: 0.7862 - val_acc: 0.5811\n",
            "Epoch 56/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.7425 - acc: 0.5525 - val_loss: 0.7853 - val_acc: 0.5811\n",
            "Epoch 57/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.7413 - acc: 0.5593 - val_loss: 0.7844 - val_acc: 0.5811\n",
            "Epoch 58/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7401 - acc: 0.5627 - val_loss: 0.7836 - val_acc: 0.5811\n",
            "Epoch 59/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7389 - acc: 0.5627 - val_loss: 0.7827 - val_acc: 0.5811\n",
            "Epoch 60/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7377 - acc: 0.5627 - val_loss: 0.7819 - val_acc: 0.5811\n",
            "Epoch 61/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7366 - acc: 0.5627 - val_loss: 0.7810 - val_acc: 0.5811\n",
            "Epoch 62/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7354 - acc: 0.5661 - val_loss: 0.7802 - val_acc: 0.5811\n",
            "Epoch 63/500\n",
            "295/295 [==============================] - 0s 35us/step - loss: 0.7343 - acc: 0.5661 - val_loss: 0.7793 - val_acc: 0.5811\n",
            "Epoch 64/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.7331 - acc: 0.5661 - val_loss: 0.7785 - val_acc: 0.5811\n",
            "Epoch 65/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.7320 - acc: 0.5661 - val_loss: 0.7776 - val_acc: 0.5811\n",
            "Epoch 66/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.7309 - acc: 0.5661 - val_loss: 0.7768 - val_acc: 0.5811\n",
            "Epoch 67/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.7298 - acc: 0.5695 - val_loss: 0.7760 - val_acc: 0.5811\n",
            "Epoch 68/500\n",
            "295/295 [==============================] - 0s 36us/step - loss: 0.7287 - acc: 0.5729 - val_loss: 0.7751 - val_acc: 0.5811\n",
            "Epoch 69/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7276 - acc: 0.5763 - val_loss: 0.7743 - val_acc: 0.5811\n",
            "Epoch 70/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7265 - acc: 0.5763 - val_loss: 0.7735 - val_acc: 0.5811\n",
            "Epoch 71/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7254 - acc: 0.5797 - val_loss: 0.7727 - val_acc: 0.5811\n",
            "Epoch 72/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7244 - acc: 0.5797 - val_loss: 0.7719 - val_acc: 0.5811\n",
            "Epoch 73/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7233 - acc: 0.5797 - val_loss: 0.7711 - val_acc: 0.5811\n",
            "Epoch 74/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7223 - acc: 0.5797 - val_loss: 0.7703 - val_acc: 0.5811\n",
            "Epoch 75/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.7213 - acc: 0.5831 - val_loss: 0.7695 - val_acc: 0.5811\n",
            "Epoch 76/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.7202 - acc: 0.5797 - val_loss: 0.7687 - val_acc: 0.5811\n",
            "Epoch 77/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7192 - acc: 0.5831 - val_loss: 0.7679 - val_acc: 0.5811\n",
            "Epoch 78/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7182 - acc: 0.5797 - val_loss: 0.7671 - val_acc: 0.5811\n",
            "Epoch 79/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7172 - acc: 0.5831 - val_loss: 0.7663 - val_acc: 0.5811\n",
            "Epoch 80/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7162 - acc: 0.5831 - val_loss: 0.7655 - val_acc: 0.5811\n",
            "Epoch 81/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.7152 - acc: 0.5831 - val_loss: 0.7648 - val_acc: 0.5811\n",
            "Epoch 82/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7143 - acc: 0.5831 - val_loss: 0.7640 - val_acc: 0.5811\n",
            "Epoch 83/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7133 - acc: 0.5864 - val_loss: 0.7632 - val_acc: 0.5946\n",
            "Epoch 84/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.7123 - acc: 0.5864 - val_loss: 0.7625 - val_acc: 0.5946\n",
            "Epoch 85/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7114 - acc: 0.5864 - val_loss: 0.7617 - val_acc: 0.5946\n",
            "Epoch 86/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7104 - acc: 0.5864 - val_loss: 0.7610 - val_acc: 0.5946\n",
            "Epoch 87/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7095 - acc: 0.5864 - val_loss: 0.7602 - val_acc: 0.5946\n",
            "Epoch 88/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7086 - acc: 0.5864 - val_loss: 0.7595 - val_acc: 0.5946\n",
            "Epoch 89/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7077 - acc: 0.5864 - val_loss: 0.7587 - val_acc: 0.5946\n",
            "Epoch 90/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7068 - acc: 0.5864 - val_loss: 0.7580 - val_acc: 0.5946\n",
            "Epoch 91/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7058 - acc: 0.5898 - val_loss: 0.7572 - val_acc: 0.5946\n",
            "Epoch 92/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7049 - acc: 0.5932 - val_loss: 0.7565 - val_acc: 0.5946\n",
            "Epoch 93/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.7041 - acc: 0.5966 - val_loss: 0.7558 - val_acc: 0.5946\n",
            "Epoch 94/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7032 - acc: 0.5966 - val_loss: 0.7550 - val_acc: 0.5946\n",
            "Epoch 95/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7023 - acc: 0.5966 - val_loss: 0.7543 - val_acc: 0.5946\n",
            "Epoch 96/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.7014 - acc: 0.6000 - val_loss: 0.7536 - val_acc: 0.5946\n",
            "Epoch 97/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.7006 - acc: 0.6000 - val_loss: 0.7529 - val_acc: 0.5946\n",
            "Epoch 98/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6997 - acc: 0.6000 - val_loss: 0.7522 - val_acc: 0.5946\n",
            "Epoch 99/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6989 - acc: 0.6034 - val_loss: 0.7514 - val_acc: 0.5946\n",
            "Epoch 100/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6980 - acc: 0.6000 - val_loss: 0.7507 - val_acc: 0.6081\n",
            "Epoch 101/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6972 - acc: 0.5966 - val_loss: 0.7500 - val_acc: 0.6081\n",
            "Epoch 102/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6963 - acc: 0.5966 - val_loss: 0.7493 - val_acc: 0.6081\n",
            "Epoch 103/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6955 - acc: 0.6000 - val_loss: 0.7486 - val_acc: 0.6081\n",
            "Epoch 104/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6947 - acc: 0.6000 - val_loss: 0.7479 - val_acc: 0.6081\n",
            "Epoch 105/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6939 - acc: 0.6000 - val_loss: 0.7472 - val_acc: 0.6081\n",
            "Epoch 106/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6931 - acc: 0.6000 - val_loss: 0.7466 - val_acc: 0.6081\n",
            "Epoch 107/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6923 - acc: 0.6034 - val_loss: 0.7459 - val_acc: 0.6081\n",
            "Epoch 108/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6915 - acc: 0.6102 - val_loss: 0.7452 - val_acc: 0.6081\n",
            "Epoch 109/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6907 - acc: 0.6102 - val_loss: 0.7445 - val_acc: 0.6081\n",
            "Epoch 110/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6899 - acc: 0.6136 - val_loss: 0.7438 - val_acc: 0.5946\n",
            "Epoch 111/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6892 - acc: 0.6136 - val_loss: 0.7432 - val_acc: 0.5946\n",
            "Epoch 112/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6884 - acc: 0.6136 - val_loss: 0.7425 - val_acc: 0.5946\n",
            "Epoch 113/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6876 - acc: 0.6136 - val_loss: 0.7418 - val_acc: 0.5946\n",
            "Epoch 114/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6869 - acc: 0.6169 - val_loss: 0.7412 - val_acc: 0.6081\n",
            "Epoch 115/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6861 - acc: 0.6169 - val_loss: 0.7405 - val_acc: 0.6081\n",
            "Epoch 116/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6854 - acc: 0.6203 - val_loss: 0.7398 - val_acc: 0.6081\n",
            "Epoch 117/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6846 - acc: 0.6203 - val_loss: 0.7392 - val_acc: 0.6081\n",
            "Epoch 118/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6839 - acc: 0.6203 - val_loss: 0.7385 - val_acc: 0.6081\n",
            "Epoch 119/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6832 - acc: 0.6203 - val_loss: 0.7379 - val_acc: 0.6081\n",
            "Epoch 120/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6825 - acc: 0.6203 - val_loss: 0.7372 - val_acc: 0.6081\n",
            "Epoch 121/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6817 - acc: 0.6203 - val_loss: 0.7366 - val_acc: 0.6081\n",
            "Epoch 122/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6810 - acc: 0.6203 - val_loss: 0.7359 - val_acc: 0.6081\n",
            "Epoch 123/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6803 - acc: 0.6237 - val_loss: 0.7353 - val_acc: 0.6081\n",
            "Epoch 124/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6796 - acc: 0.6271 - val_loss: 0.7347 - val_acc: 0.6081\n",
            "Epoch 125/500\n",
            "295/295 [==============================] - 0s 36us/step - loss: 0.6789 - acc: 0.6271 - val_loss: 0.7340 - val_acc: 0.6081\n",
            "Epoch 126/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6782 - acc: 0.6271 - val_loss: 0.7334 - val_acc: 0.6081\n",
            "Epoch 127/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6775 - acc: 0.6271 - val_loss: 0.7328 - val_acc: 0.6081\n",
            "Epoch 128/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6768 - acc: 0.6271 - val_loss: 0.7322 - val_acc: 0.6081\n",
            "Epoch 129/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6762 - acc: 0.6271 - val_loss: 0.7315 - val_acc: 0.6081\n",
            "Epoch 130/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6755 - acc: 0.6237 - val_loss: 0.7309 - val_acc: 0.6081\n",
            "Epoch 131/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6748 - acc: 0.6237 - val_loss: 0.7303 - val_acc: 0.6081\n",
            "Epoch 132/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6742 - acc: 0.6271 - val_loss: 0.7297 - val_acc: 0.6081\n",
            "Epoch 133/500\n",
            "295/295 [==============================] - 0s 55us/step - loss: 0.6735 - acc: 0.6271 - val_loss: 0.7291 - val_acc: 0.6081\n",
            "Epoch 134/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6729 - acc: 0.6271 - val_loss: 0.7285 - val_acc: 0.6081\n",
            "Epoch 135/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6722 - acc: 0.6271 - val_loss: 0.7279 - val_acc: 0.6081\n",
            "Epoch 136/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6716 - acc: 0.6305 - val_loss: 0.7272 - val_acc: 0.6081\n",
            "Epoch 137/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.6709 - acc: 0.6305 - val_loss: 0.7266 - val_acc: 0.6081\n",
            "Epoch 138/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6703 - acc: 0.6339 - val_loss: 0.7260 - val_acc: 0.6081\n",
            "Epoch 139/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6697 - acc: 0.6305 - val_loss: 0.7255 - val_acc: 0.6081\n",
            "Epoch 140/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6690 - acc: 0.6271 - val_loss: 0.7249 - val_acc: 0.5946\n",
            "Epoch 141/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6684 - acc: 0.6271 - val_loss: 0.7243 - val_acc: 0.5946\n",
            "Epoch 142/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6678 - acc: 0.6271 - val_loss: 0.7237 - val_acc: 0.5946\n",
            "Epoch 143/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6672 - acc: 0.6237 - val_loss: 0.7231 - val_acc: 0.5946\n",
            "Epoch 144/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.6666 - acc: 0.6237 - val_loss: 0.7225 - val_acc: 0.5946\n",
            "Epoch 145/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6660 - acc: 0.6237 - val_loss: 0.7219 - val_acc: 0.5946\n",
            "Epoch 146/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6654 - acc: 0.6271 - val_loss: 0.7213 - val_acc: 0.5946\n",
            "Epoch 147/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6648 - acc: 0.6271 - val_loss: 0.7208 - val_acc: 0.5946\n",
            "Epoch 148/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6642 - acc: 0.6305 - val_loss: 0.7202 - val_acc: 0.5946\n",
            "Epoch 149/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6636 - acc: 0.6305 - val_loss: 0.7196 - val_acc: 0.5946\n",
            "Epoch 150/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6630 - acc: 0.6305 - val_loss: 0.7191 - val_acc: 0.5946\n",
            "Epoch 151/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6624 - acc: 0.6305 - val_loss: 0.7185 - val_acc: 0.5946\n",
            "Epoch 152/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6618 - acc: 0.6305 - val_loss: 0.7179 - val_acc: 0.5946\n",
            "Epoch 153/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.6613 - acc: 0.6305 - val_loss: 0.7174 - val_acc: 0.5946\n",
            "Epoch 154/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6607 - acc: 0.6305 - val_loss: 0.7168 - val_acc: 0.5946\n",
            "Epoch 155/500\n",
            "295/295 [==============================] - 0s 36us/step - loss: 0.6601 - acc: 0.6339 - val_loss: 0.7162 - val_acc: 0.5946\n",
            "Epoch 156/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.6596 - acc: 0.6339 - val_loss: 0.7157 - val_acc: 0.5946\n",
            "Epoch 157/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6590 - acc: 0.6339 - val_loss: 0.7151 - val_acc: 0.5946\n",
            "Epoch 158/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6584 - acc: 0.6339 - val_loss: 0.7146 - val_acc: 0.5946\n",
            "Epoch 159/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6579 - acc: 0.6339 - val_loss: 0.7140 - val_acc: 0.5946\n",
            "Epoch 160/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6573 - acc: 0.6339 - val_loss: 0.7135 - val_acc: 0.5946\n",
            "Epoch 161/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6568 - acc: 0.6339 - val_loss: 0.7129 - val_acc: 0.5946\n",
            "Epoch 162/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6563 - acc: 0.6339 - val_loss: 0.7124 - val_acc: 0.5946\n",
            "Epoch 163/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6557 - acc: 0.6305 - val_loss: 0.7119 - val_acc: 0.5946\n",
            "Epoch 164/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6552 - acc: 0.6305 - val_loss: 0.7113 - val_acc: 0.5811\n",
            "Epoch 165/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6547 - acc: 0.6339 - val_loss: 0.7108 - val_acc: 0.5811\n",
            "Epoch 166/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6541 - acc: 0.6339 - val_loss: 0.7103 - val_acc: 0.5811\n",
            "Epoch 167/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6536 - acc: 0.6339 - val_loss: 0.7097 - val_acc: 0.5811\n",
            "Epoch 168/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6531 - acc: 0.6373 - val_loss: 0.7092 - val_acc: 0.5676\n",
            "Epoch 169/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6526 - acc: 0.6373 - val_loss: 0.7087 - val_acc: 0.5676\n",
            "Epoch 170/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6521 - acc: 0.6373 - val_loss: 0.7081 - val_acc: 0.5676\n",
            "Epoch 171/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6515 - acc: 0.6373 - val_loss: 0.7076 - val_acc: 0.5676\n",
            "Epoch 172/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6510 - acc: 0.6373 - val_loss: 0.7071 - val_acc: 0.5676\n",
            "Epoch 173/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6505 - acc: 0.6373 - val_loss: 0.7066 - val_acc: 0.5676\n",
            "Epoch 174/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6500 - acc: 0.6373 - val_loss: 0.7061 - val_acc: 0.5676\n",
            "Epoch 175/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6495 - acc: 0.6373 - val_loss: 0.7056 - val_acc: 0.5676\n",
            "Epoch 176/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6490 - acc: 0.6373 - val_loss: 0.7050 - val_acc: 0.5676\n",
            "Epoch 177/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6485 - acc: 0.6407 - val_loss: 0.7045 - val_acc: 0.5676\n",
            "Epoch 178/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6481 - acc: 0.6407 - val_loss: 0.7040 - val_acc: 0.5676\n",
            "Epoch 179/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6476 - acc: 0.6441 - val_loss: 0.7035 - val_acc: 0.5676\n",
            "Epoch 180/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6471 - acc: 0.6441 - val_loss: 0.7030 - val_acc: 0.5676\n",
            "Epoch 181/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6466 - acc: 0.6441 - val_loss: 0.7025 - val_acc: 0.5676\n",
            "Epoch 182/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6461 - acc: 0.6441 - val_loss: 0.7020 - val_acc: 0.5676\n",
            "Epoch 183/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6457 - acc: 0.6441 - val_loss: 0.7015 - val_acc: 0.5676\n",
            "Epoch 184/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6452 - acc: 0.6407 - val_loss: 0.7010 - val_acc: 0.5676\n",
            "Epoch 185/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6447 - acc: 0.6407 - val_loss: 0.7005 - val_acc: 0.5676\n",
            "Epoch 186/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6442 - acc: 0.6441 - val_loss: 0.7000 - val_acc: 0.5676\n",
            "Epoch 187/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6438 - acc: 0.6441 - val_loss: 0.6995 - val_acc: 0.5811\n",
            "Epoch 188/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6433 - acc: 0.6441 - val_loss: 0.6990 - val_acc: 0.5811\n",
            "Epoch 189/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6429 - acc: 0.6475 - val_loss: 0.6986 - val_acc: 0.5811\n",
            "Epoch 190/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6424 - acc: 0.6475 - val_loss: 0.6981 - val_acc: 0.5811\n",
            "Epoch 191/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6420 - acc: 0.6475 - val_loss: 0.6976 - val_acc: 0.5811\n",
            "Epoch 192/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6415 - acc: 0.6475 - val_loss: 0.6971 - val_acc: 0.5811\n",
            "Epoch 193/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6411 - acc: 0.6475 - val_loss: 0.6966 - val_acc: 0.5811\n",
            "Epoch 194/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.6406 - acc: 0.6475 - val_loss: 0.6962 - val_acc: 0.5811\n",
            "Epoch 195/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6402 - acc: 0.6475 - val_loss: 0.6957 - val_acc: 0.5811\n",
            "Epoch 196/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6397 - acc: 0.6508 - val_loss: 0.6952 - val_acc: 0.5811\n",
            "Epoch 197/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6393 - acc: 0.6508 - val_loss: 0.6947 - val_acc: 0.5811\n",
            "Epoch 198/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6389 - acc: 0.6508 - val_loss: 0.6943 - val_acc: 0.5811\n",
            "Epoch 199/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6384 - acc: 0.6508 - val_loss: 0.6938 - val_acc: 0.5811\n",
            "Epoch 200/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.6380 - acc: 0.6475 - val_loss: 0.6933 - val_acc: 0.5811\n",
            "Epoch 201/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6376 - acc: 0.6475 - val_loss: 0.6929 - val_acc: 0.5811\n",
            "Epoch 202/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6372 - acc: 0.6508 - val_loss: 0.6924 - val_acc: 0.5811\n",
            "Epoch 203/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6367 - acc: 0.6508 - val_loss: 0.6919 - val_acc: 0.5811\n",
            "Epoch 204/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6363 - acc: 0.6508 - val_loss: 0.6915 - val_acc: 0.5811\n",
            "Epoch 205/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6359 - acc: 0.6508 - val_loss: 0.6910 - val_acc: 0.5811\n",
            "Epoch 206/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6355 - acc: 0.6508 - val_loss: 0.6906 - val_acc: 0.5811\n",
            "Epoch 207/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6351 - acc: 0.6508 - val_loss: 0.6901 - val_acc: 0.5811\n",
            "Epoch 208/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6347 - acc: 0.6508 - val_loss: 0.6897 - val_acc: 0.5811\n",
            "Epoch 209/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6343 - acc: 0.6508 - val_loss: 0.6892 - val_acc: 0.5811\n",
            "Epoch 210/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6339 - acc: 0.6508 - val_loss: 0.6888 - val_acc: 0.5811\n",
            "Epoch 211/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6334 - acc: 0.6508 - val_loss: 0.6883 - val_acc: 0.5811\n",
            "Epoch 212/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6330 - acc: 0.6508 - val_loss: 0.6879 - val_acc: 0.5811\n",
            "Epoch 213/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.6326 - acc: 0.6508 - val_loss: 0.6874 - val_acc: 0.5811\n",
            "Epoch 214/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.6323 - acc: 0.6508 - val_loss: 0.6870 - val_acc: 0.5811\n",
            "Epoch 215/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6319 - acc: 0.6508 - val_loss: 0.6865 - val_acc: 0.5811\n",
            "Epoch 216/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6315 - acc: 0.6508 - val_loss: 0.6861 - val_acc: 0.5811\n",
            "Epoch 217/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6311 - acc: 0.6508 - val_loss: 0.6856 - val_acc: 0.5811\n",
            "Epoch 218/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6307 - acc: 0.6508 - val_loss: 0.6852 - val_acc: 0.5946\n",
            "Epoch 219/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6303 - acc: 0.6508 - val_loss: 0.6848 - val_acc: 0.5946\n",
            "Epoch 220/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6299 - acc: 0.6508 - val_loss: 0.6843 - val_acc: 0.5946\n",
            "Epoch 221/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6295 - acc: 0.6475 - val_loss: 0.6839 - val_acc: 0.5946\n",
            "Epoch 222/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6292 - acc: 0.6475 - val_loss: 0.6835 - val_acc: 0.5946\n",
            "Epoch 223/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6288 - acc: 0.6475 - val_loss: 0.6831 - val_acc: 0.5946\n",
            "Epoch 224/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6284 - acc: 0.6475 - val_loss: 0.6826 - val_acc: 0.5946\n",
            "Epoch 225/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6280 - acc: 0.6475 - val_loss: 0.6822 - val_acc: 0.5946\n",
            "Epoch 226/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6277 - acc: 0.6475 - val_loss: 0.6818 - val_acc: 0.5946\n",
            "Epoch 227/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6273 - acc: 0.6508 - val_loss: 0.6814 - val_acc: 0.5946\n",
            "Epoch 228/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6269 - acc: 0.6508 - val_loss: 0.6809 - val_acc: 0.5946\n",
            "Epoch 229/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6266 - acc: 0.6508 - val_loss: 0.6805 - val_acc: 0.5946\n",
            "Epoch 230/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6262 - acc: 0.6508 - val_loss: 0.6801 - val_acc: 0.5946\n",
            "Epoch 231/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6258 - acc: 0.6508 - val_loss: 0.6797 - val_acc: 0.5946\n",
            "Epoch 232/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.6255 - acc: 0.6542 - val_loss: 0.6793 - val_acc: 0.5946\n",
            "Epoch 233/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6251 - acc: 0.6542 - val_loss: 0.6789 - val_acc: 0.5946\n",
            "Epoch 234/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6248 - acc: 0.6542 - val_loss: 0.6784 - val_acc: 0.5946\n",
            "Epoch 235/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6244 - acc: 0.6542 - val_loss: 0.6780 - val_acc: 0.5946\n",
            "Epoch 236/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.6241 - acc: 0.6542 - val_loss: 0.6776 - val_acc: 0.5946\n",
            "Epoch 237/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6237 - acc: 0.6542 - val_loss: 0.6772 - val_acc: 0.5946\n",
            "Epoch 238/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6234 - acc: 0.6542 - val_loss: 0.6768 - val_acc: 0.5946\n",
            "Epoch 239/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6230 - acc: 0.6542 - val_loss: 0.6764 - val_acc: 0.5946\n",
            "Epoch 240/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6227 - acc: 0.6542 - val_loss: 0.6760 - val_acc: 0.5946\n",
            "Epoch 241/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6223 - acc: 0.6542 - val_loss: 0.6756 - val_acc: 0.5946\n",
            "Epoch 242/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6220 - acc: 0.6542 - val_loss: 0.6752 - val_acc: 0.5946\n",
            "Epoch 243/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6216 - acc: 0.6542 - val_loss: 0.6748 - val_acc: 0.5946\n",
            "Epoch 244/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6213 - acc: 0.6542 - val_loss: 0.6744 - val_acc: 0.5946\n",
            "Epoch 245/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6210 - acc: 0.6576 - val_loss: 0.6740 - val_acc: 0.5946\n",
            "Epoch 246/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6206 - acc: 0.6576 - val_loss: 0.6736 - val_acc: 0.5946\n",
            "Epoch 247/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6203 - acc: 0.6576 - val_loss: 0.6732 - val_acc: 0.5946\n",
            "Epoch 248/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6200 - acc: 0.6576 - val_loss: 0.6728 - val_acc: 0.5946\n",
            "Epoch 249/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6196 - acc: 0.6576 - val_loss: 0.6724 - val_acc: 0.5946\n",
            "Epoch 250/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6193 - acc: 0.6610 - val_loss: 0.6721 - val_acc: 0.5946\n",
            "Epoch 251/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6190 - acc: 0.6610 - val_loss: 0.6717 - val_acc: 0.5946\n",
            "Epoch 252/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.6187 - acc: 0.6610 - val_loss: 0.6713 - val_acc: 0.5946\n",
            "Epoch 253/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6183 - acc: 0.6610 - val_loss: 0.6709 - val_acc: 0.5946\n",
            "Epoch 254/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.6180 - acc: 0.6644 - val_loss: 0.6705 - val_acc: 0.5946\n",
            "Epoch 255/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6177 - acc: 0.6644 - val_loss: 0.6701 - val_acc: 0.5946\n",
            "Epoch 256/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6174 - acc: 0.6644 - val_loss: 0.6698 - val_acc: 0.5946\n",
            "Epoch 257/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6171 - acc: 0.6644 - val_loss: 0.6694 - val_acc: 0.5946\n",
            "Epoch 258/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6167 - acc: 0.6644 - val_loss: 0.6690 - val_acc: 0.5946\n",
            "Epoch 259/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6164 - acc: 0.6644 - val_loss: 0.6686 - val_acc: 0.5946\n",
            "Epoch 260/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6161 - acc: 0.6644 - val_loss: 0.6683 - val_acc: 0.5946\n",
            "Epoch 261/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6158 - acc: 0.6644 - val_loss: 0.6679 - val_acc: 0.5946\n",
            "Epoch 262/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6155 - acc: 0.6644 - val_loss: 0.6675 - val_acc: 0.5946\n",
            "Epoch 263/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6152 - acc: 0.6644 - val_loss: 0.6671 - val_acc: 0.5946\n",
            "Epoch 264/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6149 - acc: 0.6610 - val_loss: 0.6668 - val_acc: 0.5946\n",
            "Epoch 265/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6146 - acc: 0.6610 - val_loss: 0.6664 - val_acc: 0.5946\n",
            "Epoch 266/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6143 - acc: 0.6610 - val_loss: 0.6660 - val_acc: 0.5946\n",
            "Epoch 267/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6140 - acc: 0.6610 - val_loss: 0.6657 - val_acc: 0.5946\n",
            "Epoch 268/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6137 - acc: 0.6610 - val_loss: 0.6653 - val_acc: 0.5946\n",
            "Epoch 269/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6134 - acc: 0.6610 - val_loss: 0.6649 - val_acc: 0.5946\n",
            "Epoch 270/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6131 - acc: 0.6610 - val_loss: 0.6646 - val_acc: 0.5946\n",
            "Epoch 271/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6128 - acc: 0.6610 - val_loss: 0.6642 - val_acc: 0.5946\n",
            "Epoch 272/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6125 - acc: 0.6610 - val_loss: 0.6639 - val_acc: 0.5946\n",
            "Epoch 273/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6122 - acc: 0.6610 - val_loss: 0.6635 - val_acc: 0.5946\n",
            "Epoch 274/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6119 - acc: 0.6610 - val_loss: 0.6631 - val_acc: 0.5946\n",
            "Epoch 275/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6116 - acc: 0.6644 - val_loss: 0.6628 - val_acc: 0.5946\n",
            "Epoch 276/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6113 - acc: 0.6610 - val_loss: 0.6624 - val_acc: 0.5946\n",
            "Epoch 277/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6110 - acc: 0.6610 - val_loss: 0.6621 - val_acc: 0.5946\n",
            "Epoch 278/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6107 - acc: 0.6610 - val_loss: 0.6617 - val_acc: 0.5946\n",
            "Epoch 279/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6105 - acc: 0.6610 - val_loss: 0.6614 - val_acc: 0.5946\n",
            "Epoch 280/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6102 - acc: 0.6610 - val_loss: 0.6610 - val_acc: 0.5946\n",
            "Epoch 281/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6099 - acc: 0.6644 - val_loss: 0.6607 - val_acc: 0.5946\n",
            "Epoch 282/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6096 - acc: 0.6644 - val_loss: 0.6603 - val_acc: 0.5946\n",
            "Epoch 283/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6093 - acc: 0.6644 - val_loss: 0.6600 - val_acc: 0.5946\n",
            "Epoch 284/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6091 - acc: 0.6644 - val_loss: 0.6596 - val_acc: 0.5946\n",
            "Epoch 285/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6088 - acc: 0.6678 - val_loss: 0.6593 - val_acc: 0.5946\n",
            "Epoch 286/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.6085 - acc: 0.6678 - val_loss: 0.6590 - val_acc: 0.5946\n",
            "Epoch 287/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6082 - acc: 0.6678 - val_loss: 0.6586 - val_acc: 0.5946\n",
            "Epoch 288/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6079 - acc: 0.6678 - val_loss: 0.6583 - val_acc: 0.6081\n",
            "Epoch 289/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6077 - acc: 0.6678 - val_loss: 0.6579 - val_acc: 0.6081\n",
            "Epoch 290/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6074 - acc: 0.6678 - val_loss: 0.6576 - val_acc: 0.6081\n",
            "Epoch 291/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6071 - acc: 0.6712 - val_loss: 0.6573 - val_acc: 0.6081\n",
            "Epoch 292/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6069 - acc: 0.6712 - val_loss: 0.6569 - val_acc: 0.6081\n",
            "Epoch 293/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6066 - acc: 0.6712 - val_loss: 0.6566 - val_acc: 0.6081\n",
            "Epoch 294/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6063 - acc: 0.6712 - val_loss: 0.6563 - val_acc: 0.6081\n",
            "Epoch 295/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6061 - acc: 0.6712 - val_loss: 0.6559 - val_acc: 0.6081\n",
            "Epoch 296/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6058 - acc: 0.6712 - val_loss: 0.6556 - val_acc: 0.6081\n",
            "Epoch 297/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6055 - acc: 0.6678 - val_loss: 0.6553 - val_acc: 0.6081\n",
            "Epoch 298/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6053 - acc: 0.6678 - val_loss: 0.6549 - val_acc: 0.6081\n",
            "Epoch 299/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6050 - acc: 0.6712 - val_loss: 0.6546 - val_acc: 0.6081\n",
            "Epoch 300/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6048 - acc: 0.6712 - val_loss: 0.6543 - val_acc: 0.6081\n",
            "Epoch 301/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6045 - acc: 0.6712 - val_loss: 0.6540 - val_acc: 0.6081\n",
            "Epoch 302/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6042 - acc: 0.6712 - val_loss: 0.6536 - val_acc: 0.6081\n",
            "Epoch 303/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6040 - acc: 0.6712 - val_loss: 0.6533 - val_acc: 0.6081\n",
            "Epoch 304/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6037 - acc: 0.6746 - val_loss: 0.6530 - val_acc: 0.6081\n",
            "Epoch 305/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6035 - acc: 0.6746 - val_loss: 0.6527 - val_acc: 0.6081\n",
            "Epoch 306/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6032 - acc: 0.6746 - val_loss: 0.6524 - val_acc: 0.6081\n",
            "Epoch 307/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6030 - acc: 0.6746 - val_loss: 0.6520 - val_acc: 0.6081\n",
            "Epoch 308/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6027 - acc: 0.6746 - val_loss: 0.6517 - val_acc: 0.6081\n",
            "Epoch 309/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6025 - acc: 0.6746 - val_loss: 0.6514 - val_acc: 0.6081\n",
            "Epoch 310/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6022 - acc: 0.6746 - val_loss: 0.6511 - val_acc: 0.6081\n",
            "Epoch 311/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6020 - acc: 0.6746 - val_loss: 0.6508 - val_acc: 0.6081\n",
            "Epoch 312/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6017 - acc: 0.6746 - val_loss: 0.6505 - val_acc: 0.6081\n",
            "Epoch 313/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6015 - acc: 0.6780 - val_loss: 0.6501 - val_acc: 0.6081\n",
            "Epoch 314/500\n",
            "295/295 [==============================] - 0s 33us/step - loss: 0.6012 - acc: 0.6780 - val_loss: 0.6498 - val_acc: 0.6081\n",
            "Epoch 315/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6010 - acc: 0.6780 - val_loss: 0.6495 - val_acc: 0.6081\n",
            "Epoch 316/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6008 - acc: 0.6780 - val_loss: 0.6492 - val_acc: 0.6081\n",
            "Epoch 317/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6005 - acc: 0.6780 - val_loss: 0.6489 - val_acc: 0.6081\n",
            "Epoch 318/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6003 - acc: 0.6780 - val_loss: 0.6486 - val_acc: 0.6081\n",
            "Epoch 319/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6000 - acc: 0.6780 - val_loss: 0.6483 - val_acc: 0.6081\n",
            "Epoch 320/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5998 - acc: 0.6780 - val_loss: 0.6480 - val_acc: 0.6081\n",
            "Epoch 321/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5996 - acc: 0.6780 - val_loss: 0.6477 - val_acc: 0.6081\n",
            "Epoch 322/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5993 - acc: 0.6814 - val_loss: 0.6474 - val_acc: 0.6081\n",
            "Epoch 323/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5991 - acc: 0.6814 - val_loss: 0.6471 - val_acc: 0.6081\n",
            "Epoch 324/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.5989 - acc: 0.6847 - val_loss: 0.6468 - val_acc: 0.6081\n",
            "Epoch 325/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5986 - acc: 0.6847 - val_loss: 0.6465 - val_acc: 0.6081\n",
            "Epoch 326/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5984 - acc: 0.6847 - val_loss: 0.6462 - val_acc: 0.6081\n",
            "Epoch 327/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5982 - acc: 0.6847 - val_loss: 0.6459 - val_acc: 0.6081\n",
            "Epoch 328/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5979 - acc: 0.6847 - val_loss: 0.6456 - val_acc: 0.6081\n",
            "Epoch 329/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5977 - acc: 0.6847 - val_loss: 0.6453 - val_acc: 0.6081\n",
            "Epoch 330/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5975 - acc: 0.6847 - val_loss: 0.6450 - val_acc: 0.6081\n",
            "Epoch 331/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5973 - acc: 0.6847 - val_loss: 0.6447 - val_acc: 0.6081\n",
            "Epoch 332/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5970 - acc: 0.6847 - val_loss: 0.6444 - val_acc: 0.6081\n",
            "Epoch 333/500\n",
            "295/295 [==============================] - 0s 38us/step - loss: 0.5968 - acc: 0.6847 - val_loss: 0.6441 - val_acc: 0.6081\n",
            "Epoch 334/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5966 - acc: 0.6814 - val_loss: 0.6438 - val_acc: 0.6081\n",
            "Epoch 335/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5964 - acc: 0.6780 - val_loss: 0.6435 - val_acc: 0.6081\n",
            "Epoch 336/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5961 - acc: 0.6780 - val_loss: 0.6432 - val_acc: 0.6081\n",
            "Epoch 337/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5959 - acc: 0.6780 - val_loss: 0.6430 - val_acc: 0.6081\n",
            "Epoch 338/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5957 - acc: 0.6780 - val_loss: 0.6427 - val_acc: 0.6081\n",
            "Epoch 339/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5955 - acc: 0.6814 - val_loss: 0.6424 - val_acc: 0.6081\n",
            "Epoch 340/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5952 - acc: 0.6814 - val_loss: 0.6421 - val_acc: 0.6081\n",
            "Epoch 341/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5950 - acc: 0.6814 - val_loss: 0.6418 - val_acc: 0.6081\n",
            "Epoch 342/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5948 - acc: 0.6814 - val_loss: 0.6415 - val_acc: 0.6081\n",
            "Epoch 343/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5946 - acc: 0.6814 - val_loss: 0.6413 - val_acc: 0.6081\n",
            "Epoch 344/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5944 - acc: 0.6814 - val_loss: 0.6410 - val_acc: 0.6081\n",
            "Epoch 345/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5942 - acc: 0.6814 - val_loss: 0.6407 - val_acc: 0.6081\n",
            "Epoch 346/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5940 - acc: 0.6847 - val_loss: 0.6404 - val_acc: 0.6081\n",
            "Epoch 347/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5937 - acc: 0.6847 - val_loss: 0.6401 - val_acc: 0.6081\n",
            "Epoch 348/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5935 - acc: 0.6847 - val_loss: 0.6399 - val_acc: 0.6081\n",
            "Epoch 349/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5933 - acc: 0.6847 - val_loss: 0.6396 - val_acc: 0.6081\n",
            "Epoch 350/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5931 - acc: 0.6847 - val_loss: 0.6393 - val_acc: 0.6081\n",
            "Epoch 351/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5929 - acc: 0.6847 - val_loss: 0.6390 - val_acc: 0.6081\n",
            "Epoch 352/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5927 - acc: 0.6881 - val_loss: 0.6388 - val_acc: 0.6081\n",
            "Epoch 353/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5925 - acc: 0.6881 - val_loss: 0.6385 - val_acc: 0.6081\n",
            "Epoch 354/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5923 - acc: 0.6881 - val_loss: 0.6382 - val_acc: 0.6081\n",
            "Epoch 355/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5921 - acc: 0.6881 - val_loss: 0.6379 - val_acc: 0.6081\n",
            "Epoch 356/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5919 - acc: 0.6881 - val_loss: 0.6377 - val_acc: 0.6081\n",
            "Epoch 357/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5917 - acc: 0.6881 - val_loss: 0.6374 - val_acc: 0.6216\n",
            "Epoch 358/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5915 - acc: 0.6881 - val_loss: 0.6371 - val_acc: 0.6216\n",
            "Epoch 359/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5913 - acc: 0.6881 - val_loss: 0.6369 - val_acc: 0.6216\n",
            "Epoch 360/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5911 - acc: 0.6881 - val_loss: 0.6366 - val_acc: 0.6216\n",
            "Epoch 361/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5909 - acc: 0.6881 - val_loss: 0.6363 - val_acc: 0.6216\n",
            "Epoch 362/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5907 - acc: 0.6881 - val_loss: 0.6361 - val_acc: 0.6216\n",
            "Epoch 363/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5905 - acc: 0.6881 - val_loss: 0.6358 - val_acc: 0.6216\n",
            "Epoch 364/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5903 - acc: 0.6881 - val_loss: 0.6355 - val_acc: 0.6216\n",
            "Epoch 365/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5901 - acc: 0.6881 - val_loss: 0.6353 - val_acc: 0.6216\n",
            "Epoch 366/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5899 - acc: 0.6881 - val_loss: 0.6350 - val_acc: 0.6351\n",
            "Epoch 367/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5897 - acc: 0.6847 - val_loss: 0.6348 - val_acc: 0.6351\n",
            "Epoch 368/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5895 - acc: 0.6847 - val_loss: 0.6345 - val_acc: 0.6486\n",
            "Epoch 369/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5893 - acc: 0.6881 - val_loss: 0.6342 - val_acc: 0.6486\n",
            "Epoch 370/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5891 - acc: 0.6881 - val_loss: 0.6340 - val_acc: 0.6486\n",
            "Epoch 371/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5889 - acc: 0.6881 - val_loss: 0.6337 - val_acc: 0.6486\n",
            "Epoch 372/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5887 - acc: 0.6847 - val_loss: 0.6335 - val_acc: 0.6486\n",
            "Epoch 373/500\n",
            "295/295 [==============================] - 0s 41us/step - loss: 0.5885 - acc: 0.6847 - val_loss: 0.6332 - val_acc: 0.6486\n",
            "Epoch 374/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5883 - acc: 0.6881 - val_loss: 0.6329 - val_acc: 0.6486\n",
            "Epoch 375/500\n",
            "295/295 [==============================] - 0s 61us/step - loss: 0.5881 - acc: 0.6881 - val_loss: 0.6327 - val_acc: 0.6486\n",
            "Epoch 376/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5879 - acc: 0.6881 - val_loss: 0.6324 - val_acc: 0.6486\n",
            "Epoch 377/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5877 - acc: 0.6881 - val_loss: 0.6322 - val_acc: 0.6486\n",
            "Epoch 378/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5876 - acc: 0.6881 - val_loss: 0.6319 - val_acc: 0.6486\n",
            "Epoch 379/500\n",
            "295/295 [==============================] - 0s 33us/step - loss: 0.5874 - acc: 0.6881 - val_loss: 0.6317 - val_acc: 0.6486\n",
            "Epoch 380/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5872 - acc: 0.6881 - val_loss: 0.6314 - val_acc: 0.6486\n",
            "Epoch 381/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5870 - acc: 0.6881 - val_loss: 0.6312 - val_acc: 0.6486\n",
            "Epoch 382/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5868 - acc: 0.6881 - val_loss: 0.6309 - val_acc: 0.6486\n",
            "Epoch 383/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5866 - acc: 0.6881 - val_loss: 0.6307 - val_acc: 0.6486\n",
            "Epoch 384/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5864 - acc: 0.6915 - val_loss: 0.6304 - val_acc: 0.6486\n",
            "Epoch 385/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5863 - acc: 0.6915 - val_loss: 0.6302 - val_acc: 0.6486\n",
            "Epoch 386/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5861 - acc: 0.6915 - val_loss: 0.6300 - val_acc: 0.6486\n",
            "Epoch 387/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5859 - acc: 0.6915 - val_loss: 0.6297 - val_acc: 0.6486\n",
            "Epoch 388/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5857 - acc: 0.6915 - val_loss: 0.6295 - val_acc: 0.6486\n",
            "Epoch 389/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5855 - acc: 0.6915 - val_loss: 0.6292 - val_acc: 0.6486\n",
            "Epoch 390/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5853 - acc: 0.6915 - val_loss: 0.6290 - val_acc: 0.6486\n",
            "Epoch 391/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5852 - acc: 0.6915 - val_loss: 0.6287 - val_acc: 0.6486\n",
            "Epoch 392/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5850 - acc: 0.6915 - val_loss: 0.6285 - val_acc: 0.6486\n",
            "Epoch 393/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5848 - acc: 0.6915 - val_loss: 0.6283 - val_acc: 0.6486\n",
            "Epoch 394/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5846 - acc: 0.6915 - val_loss: 0.6280 - val_acc: 0.6486\n",
            "Epoch 395/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5845 - acc: 0.6915 - val_loss: 0.6278 - val_acc: 0.6486\n",
            "Epoch 396/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5843 - acc: 0.6915 - val_loss: 0.6275 - val_acc: 0.6486\n",
            "Epoch 397/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5841 - acc: 0.6915 - val_loss: 0.6273 - val_acc: 0.6486\n",
            "Epoch 398/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5839 - acc: 0.6915 - val_loss: 0.6271 - val_acc: 0.6486\n",
            "Epoch 399/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5838 - acc: 0.6915 - val_loss: 0.6268 - val_acc: 0.6486\n",
            "Epoch 400/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5836 - acc: 0.6915 - val_loss: 0.6266 - val_acc: 0.6486\n",
            "Epoch 401/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5834 - acc: 0.6915 - val_loss: 0.6264 - val_acc: 0.6486\n",
            "Epoch 402/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5832 - acc: 0.6915 - val_loss: 0.6261 - val_acc: 0.6486\n",
            "Epoch 403/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5831 - acc: 0.6949 - val_loss: 0.6259 - val_acc: 0.6622\n",
            "Epoch 404/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5829 - acc: 0.6949 - val_loss: 0.6257 - val_acc: 0.6622\n",
            "Epoch 405/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5827 - acc: 0.6949 - val_loss: 0.6254 - val_acc: 0.6622\n",
            "Epoch 406/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5826 - acc: 0.6949 - val_loss: 0.6252 - val_acc: 0.6757\n",
            "Epoch 407/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5824 - acc: 0.6949 - val_loss: 0.6250 - val_acc: 0.6757\n",
            "Epoch 408/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5822 - acc: 0.6949 - val_loss: 0.6248 - val_acc: 0.6757\n",
            "Epoch 409/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5821 - acc: 0.6949 - val_loss: 0.6245 - val_acc: 0.6757\n",
            "Epoch 410/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5819 - acc: 0.6949 - val_loss: 0.6243 - val_acc: 0.6757\n",
            "Epoch 411/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5817 - acc: 0.6949 - val_loss: 0.6241 - val_acc: 0.6757\n",
            "Epoch 412/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5816 - acc: 0.6915 - val_loss: 0.6239 - val_acc: 0.6757\n",
            "Epoch 413/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5814 - acc: 0.6915 - val_loss: 0.6236 - val_acc: 0.6757\n",
            "Epoch 414/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5812 - acc: 0.6915 - val_loss: 0.6234 - val_acc: 0.6757\n",
            "Epoch 415/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5811 - acc: 0.6915 - val_loss: 0.6232 - val_acc: 0.6757\n",
            "Epoch 416/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5809 - acc: 0.6915 - val_loss: 0.6230 - val_acc: 0.6757\n",
            "Epoch 417/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5807 - acc: 0.6915 - val_loss: 0.6227 - val_acc: 0.6757\n",
            "Epoch 418/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5806 - acc: 0.6915 - val_loss: 0.6225 - val_acc: 0.6757\n",
            "Epoch 419/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5804 - acc: 0.6915 - val_loss: 0.6223 - val_acc: 0.6757\n",
            "Epoch 420/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5803 - acc: 0.6915 - val_loss: 0.6221 - val_acc: 0.6757\n",
            "Epoch 421/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5801 - acc: 0.6915 - val_loss: 0.6219 - val_acc: 0.6757\n",
            "Epoch 422/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5799 - acc: 0.6915 - val_loss: 0.6216 - val_acc: 0.6757\n",
            "Epoch 423/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5798 - acc: 0.6915 - val_loss: 0.6214 - val_acc: 0.6757\n",
            "Epoch 424/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5796 - acc: 0.6915 - val_loss: 0.6212 - val_acc: 0.6757\n",
            "Epoch 425/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5795 - acc: 0.6915 - val_loss: 0.6210 - val_acc: 0.6757\n",
            "Epoch 426/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5793 - acc: 0.6881 - val_loss: 0.6208 - val_acc: 0.6757\n",
            "Epoch 427/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5791 - acc: 0.6881 - val_loss: 0.6206 - val_acc: 0.6757\n",
            "Epoch 428/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5790 - acc: 0.6881 - val_loss: 0.6203 - val_acc: 0.6757\n",
            "Epoch 429/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5788 - acc: 0.6881 - val_loss: 0.6201 - val_acc: 0.6757\n",
            "Epoch 430/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5787 - acc: 0.6881 - val_loss: 0.6199 - val_acc: 0.6757\n",
            "Epoch 431/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5785 - acc: 0.6881 - val_loss: 0.6197 - val_acc: 0.6757\n",
            "Epoch 432/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5784 - acc: 0.6881 - val_loss: 0.6195 - val_acc: 0.6757\n",
            "Epoch 433/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5782 - acc: 0.6881 - val_loss: 0.6193 - val_acc: 0.6757\n",
            "Epoch 434/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5781 - acc: 0.6881 - val_loss: 0.6191 - val_acc: 0.6757\n",
            "Epoch 435/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5779 - acc: 0.6881 - val_loss: 0.6189 - val_acc: 0.6757\n",
            "Epoch 436/500\n",
            "295/295 [==============================] - 0s 39us/step - loss: 0.5778 - acc: 0.6881 - val_loss: 0.6187 - val_acc: 0.6757\n",
            "Epoch 437/500\n",
            "295/295 [==============================] - 0s 43us/step - loss: 0.5776 - acc: 0.6881 - val_loss: 0.6184 - val_acc: 0.6757\n",
            "Epoch 438/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5774 - acc: 0.6881 - val_loss: 0.6182 - val_acc: 0.6757\n",
            "Epoch 439/500\n",
            "295/295 [==============================] - 0s 36us/step - loss: 0.5773 - acc: 0.6881 - val_loss: 0.6180 - val_acc: 0.6757\n",
            "Epoch 440/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5771 - acc: 0.6881 - val_loss: 0.6178 - val_acc: 0.6757\n",
            "Epoch 441/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5770 - acc: 0.6881 - val_loss: 0.6176 - val_acc: 0.6757\n",
            "Epoch 442/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5769 - acc: 0.6881 - val_loss: 0.6174 - val_acc: 0.6622\n",
            "Epoch 443/500\n",
            "295/295 [==============================] - 0s 35us/step - loss: 0.5767 - acc: 0.6881 - val_loss: 0.6172 - val_acc: 0.6622\n",
            "Epoch 444/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5766 - acc: 0.6881 - val_loss: 0.6170 - val_acc: 0.6622\n",
            "Epoch 445/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5764 - acc: 0.6881 - val_loss: 0.6168 - val_acc: 0.6622\n",
            "Epoch 446/500\n",
            "295/295 [==============================] - 0s 43us/step - loss: 0.5763 - acc: 0.6881 - val_loss: 0.6166 - val_acc: 0.6622\n",
            "Epoch 447/500\n",
            "295/295 [==============================] - 0s 33us/step - loss: 0.5761 - acc: 0.6881 - val_loss: 0.6164 - val_acc: 0.6622\n",
            "Epoch 448/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5760 - acc: 0.6881 - val_loss: 0.6162 - val_acc: 0.6622\n",
            "Epoch 449/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5758 - acc: 0.6881 - val_loss: 0.6160 - val_acc: 0.6622\n",
            "Epoch 450/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5757 - acc: 0.6881 - val_loss: 0.6158 - val_acc: 0.6622\n",
            "Epoch 451/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5755 - acc: 0.6881 - val_loss: 0.6156 - val_acc: 0.6622\n",
            "Epoch 452/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5754 - acc: 0.6847 - val_loss: 0.6154 - val_acc: 0.6622\n",
            "Epoch 453/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5752 - acc: 0.6847 - val_loss: 0.6152 - val_acc: 0.6622\n",
            "Epoch 454/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5751 - acc: 0.6847 - val_loss: 0.6150 - val_acc: 0.6622\n",
            "Epoch 455/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5750 - acc: 0.6847 - val_loss: 0.6148 - val_acc: 0.6622\n",
            "Epoch 456/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5748 - acc: 0.6847 - val_loss: 0.6146 - val_acc: 0.6622\n",
            "Epoch 457/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5747 - acc: 0.6847 - val_loss: 0.6144 - val_acc: 0.6622\n",
            "Epoch 458/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5745 - acc: 0.6881 - val_loss: 0.6142 - val_acc: 0.6622\n",
            "Epoch 459/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5744 - acc: 0.6881 - val_loss: 0.6140 - val_acc: 0.6622\n",
            "Epoch 460/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5743 - acc: 0.6881 - val_loss: 0.6138 - val_acc: 0.6622\n",
            "Epoch 461/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5741 - acc: 0.6847 - val_loss: 0.6136 - val_acc: 0.6622\n",
            "Epoch 462/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5740 - acc: 0.6847 - val_loss: 0.6134 - val_acc: 0.6622\n",
            "Epoch 463/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5738 - acc: 0.6881 - val_loss: 0.6133 - val_acc: 0.6622\n",
            "Epoch 464/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.5737 - acc: 0.6881 - val_loss: 0.6131 - val_acc: 0.6622\n",
            "Epoch 465/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5736 - acc: 0.6881 - val_loss: 0.6129 - val_acc: 0.6622\n",
            "Epoch 466/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5734 - acc: 0.6881 - val_loss: 0.6127 - val_acc: 0.6622\n",
            "Epoch 467/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.5733 - acc: 0.6881 - val_loss: 0.6125 - val_acc: 0.6622\n",
            "Epoch 468/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5732 - acc: 0.6881 - val_loss: 0.6123 - val_acc: 0.6622\n",
            "Epoch 469/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5730 - acc: 0.6881 - val_loss: 0.6121 - val_acc: 0.6622\n",
            "Epoch 470/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5729 - acc: 0.6881 - val_loss: 0.6119 - val_acc: 0.6622\n",
            "Epoch 471/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5728 - acc: 0.6847 - val_loss: 0.6117 - val_acc: 0.6622\n",
            "Epoch 472/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5726 - acc: 0.6847 - val_loss: 0.6116 - val_acc: 0.6622\n",
            "Epoch 473/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5725 - acc: 0.6847 - val_loss: 0.6114 - val_acc: 0.6622\n",
            "Epoch 474/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5724 - acc: 0.6847 - val_loss: 0.6112 - val_acc: 0.6622\n",
            "Epoch 475/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5722 - acc: 0.6847 - val_loss: 0.6110 - val_acc: 0.6622\n",
            "Epoch 476/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5721 - acc: 0.6847 - val_loss: 0.6108 - val_acc: 0.6622\n",
            "Epoch 477/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5720 - acc: 0.6847 - val_loss: 0.6106 - val_acc: 0.6622\n",
            "Epoch 478/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5718 - acc: 0.6847 - val_loss: 0.6105 - val_acc: 0.6622\n",
            "Epoch 479/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5717 - acc: 0.6847 - val_loss: 0.6103 - val_acc: 0.6622\n",
            "Epoch 480/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5716 - acc: 0.6847 - val_loss: 0.6101 - val_acc: 0.6622\n",
            "Epoch 481/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5714 - acc: 0.6881 - val_loss: 0.6099 - val_acc: 0.6622\n",
            "Epoch 482/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5713 - acc: 0.6881 - val_loss: 0.6097 - val_acc: 0.6622\n",
            "Epoch 483/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5712 - acc: 0.6881 - val_loss: 0.6095 - val_acc: 0.6622\n",
            "Epoch 484/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5710 - acc: 0.6881 - val_loss: 0.6094 - val_acc: 0.6622\n",
            "Epoch 485/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5709 - acc: 0.6881 - val_loss: 0.6092 - val_acc: 0.6622\n",
            "Epoch 486/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5708 - acc: 0.6881 - val_loss: 0.6090 - val_acc: 0.6622\n",
            "Epoch 487/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5707 - acc: 0.6881 - val_loss: 0.6088 - val_acc: 0.6622\n",
            "Epoch 488/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5705 - acc: 0.6881 - val_loss: 0.6087 - val_acc: 0.6622\n",
            "Epoch 489/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5704 - acc: 0.6881 - val_loss: 0.6085 - val_acc: 0.6622\n",
            "Epoch 490/500\n",
            "295/295 [==============================] - 0s 37us/step - loss: 0.5703 - acc: 0.6881 - val_loss: 0.6083 - val_acc: 0.6622\n",
            "Epoch 491/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5702 - acc: 0.6881 - val_loss: 0.6081 - val_acc: 0.6622\n",
            "Epoch 492/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.5700 - acc: 0.6881 - val_loss: 0.6080 - val_acc: 0.6622\n",
            "Epoch 493/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5699 - acc: 0.6881 - val_loss: 0.6078 - val_acc: 0.6622\n",
            "Epoch 494/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5698 - acc: 0.6847 - val_loss: 0.6076 - val_acc: 0.6622\n",
            "Epoch 495/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5697 - acc: 0.6847 - val_loss: 0.6074 - val_acc: 0.6622\n",
            "Epoch 496/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5695 - acc: 0.6847 - val_loss: 0.6073 - val_acc: 0.6622\n",
            "Epoch 497/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5694 - acc: 0.6847 - val_loss: 0.6071 - val_acc: 0.6622\n",
            "Epoch 498/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5693 - acc: 0.6847 - val_loss: 0.6069 - val_acc: 0.6622\n",
            "Epoch 499/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5692 - acc: 0.6847 - val_loss: 0.6067 - val_acc: 0.6622\n",
            "Epoch 500/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5690 - acc: 0.6847 - val_loss: 0.6066 - val_acc: 0.6622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCn67LWbWUpf",
        "colab_type": "text"
      },
      "source": [
        "Check out your training set loss and accuracy and your validation set loss and accuracy.  How do they compare?  Does it look like the model has overfit the training data, or are we ok?  (You don't need to write anything, just think about this for a few seconds.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyFu0I_eR5Kk",
        "colab_type": "text"
      },
      "source": [
        "**Evan's thoughts r.e. overfitting**: We would have overfit the training data if performance is much better on the training data than on a separate data set (namely, the validation data).  In this case, at the end of the training process the training set accuracy (0.6847) is larger than the validation set accuracy (0.6622), so there is not evidence that we have overfit the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4mSUe7ZQYl",
        "colab_type": "text"
      },
      "source": [
        "# Use the model fit to make predictions.\n",
        "\n",
        "### 2. Calculate test set predictions.\n",
        "\n",
        "As we've seen, once we have an estimated bias $b$ and weights $w$, the activations for a single observation are calculated as\n",
        "\n",
        "\\begin{align*}\n",
        "z^{(i)} &= b + w^T x^{(i)} = b + \\left(x^{(i)}\\right)^T w \\\\\n",
        "a^{(i)} &= \\frac{\\exp\\left(z^{(i)}\\right)}{1 + \\exp\\left(z^{(i)}\\right)}\n",
        "\\end{align*}\n",
        "\n",
        "If we have $m$ observations with corresponding column vectors of features $x^{(1)}, \\ldots, x^{(m)}$, we can organize the first step of this calculation in either of two ways:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} z^{(1)} & z^{(2)} & \\cdots & z^{(m)}\\end{bmatrix} = \\begin{bmatrix} b & b & \\cdots & b \\end{bmatrix} + w^T \\begin{bmatrix} {x^{(1)}} & {x^{(2)}} & \\cdots & {x^{(m)}} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "...or...\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} z^{(1)} \\\\ z^{(2)} \\\\ \\vdots \\\\ z^{(m)}\\end{bmatrix} = \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix} + \\begin{bmatrix} {x^{(1)}}^T \\\\ {x^{(2)}}^T \\\\ \\vdots \\\\ {x^{(m)}}^T \\end{bmatrix} w\n",
        "$$\n",
        "\n",
        "Note that these equations are equivalent; the second is just the transpose of the first.  Andrew Ng's video lectures use the first way of organizing this, and our textbook uses the second.\n",
        "\n",
        "Either way, we then apply the sigmoid activation function elementwise to the vector of $z$'s and predict that a test set observation $y^{(i)} = 1$ if $a^{(i)} \\geq 0.5$.\n",
        "\n",
        "Implement this calculation in the space below (maybe you'd like to be sure you know how to do it both ways outlined above?):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbxrViDOiSZz",
        "colab_type": "code",
        "outputId": "6018a73e-8129-4e33-da25-22c7eb2ab10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "# This line extracts the estimated weights w and bias b from the model fit\n",
        "(w, b) = logistic_model.layers[0].get_weights()\n",
        "\n",
        "# Calculate the vector z here:\n",
        "# This should involve b, X_test, and w.  Use np.dot() and broadcasting.\n",
        "# Do you need to find any transposes?\n",
        "z = b + np.dot(X_test, w)\n",
        "# OR\n",
        "z = b + np.dot(w.T, X_test.T)\n",
        "\n",
        "# Calculate the activation vector a:\n",
        "a = np.exp(z) / (1 + np.exp(z)) # you'll want to use np.exp()\n",
        "print(a)\n",
        "\n",
        "# Find the predicted value y hat, as a logical (lgl) vector\n",
        "y_hat_lgl = (a >= 0.5)\n",
        "\n",
        "# Convert y_hat_logical to a float using the astype function from NumPy.\n",
        "y_hat = y_hat_lgl.astype(float)\n",
        "\n",
        "# For each prediction, determine whether the prediction was correct by\n",
        "# comparing it to the observed test set response.  The result of this\n",
        "# calculation should be a logical vector of the same shape as y_test\n",
        "# that is True for cases where the test set prediction was correct and False\n",
        "# for test set cases where the prediction was wrong.\n",
        "# Careful!  Make sure y_hat and y_test have the same shape before comparing\n",
        "# them, or else you'll accidentally broadcast the comparison and be confused...\n",
        "y_hat_correct_lgl = (y_hat.T == y_test)\n",
        "\n",
        "# Determine what proportion of your test set predictions were correct by\n",
        "# calculating the *mean* of the values in the y_hat_correct_lgl variable.\n",
        "# Note that any True values (correct predictions) are converted to 1 and False\n",
        "# values are converted to 0 when you do this calculation.\n",
        "proportion_correct = np.mean(y_hat_correct_lgl)\n",
        "\n",
        "# Print the proportion correct so you can see it after running this code cell\n",
        "print(\"Proportion correct = \" + str(proportion_correct))\n",
        "\n",
        "# The following code does all of the above automagically by calling the keras\n",
        "# function evaluate.  The first number in the output from evaluate is the loss,\n",
        "# and the second is the classification accuracy.  As a check, your calculation\n",
        "# of the classification accuracy should match up with what evaluate says.\n",
        "# Aren't you glad you know how it works?\n",
        "print(logistic_model.evaluate(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.44498244 0.28986427 0.71807239 0.46060757 0.54617593 0.30763257\n",
            "  0.32226048 0.3937339  0.31464813 0.31668248 0.35660886 0.86092035\n",
            "  0.261676   0.71082998 0.43661601 0.58614533 0.18529509 0.31213054\n",
            "  0.77893013 0.89845647 0.43617911 0.47978332 0.16033109 0.71963046\n",
            "  0.38304318 0.40104278 0.29839463 0.26157988 0.66015134 0.29262123\n",
            "  0.42485041 0.17297187 0.24671958 0.60118214 0.1645723  0.74543758\n",
            "  0.86012083 0.17851617 0.72544951 0.57957314 0.31035407 0.74691853\n",
            "  0.63201462 0.22397979 0.27734178 0.53326073 0.34135738 0.5220477\n",
            "  0.30056091 0.26316402 0.32668188 0.25100771 0.26975509 0.19380466\n",
            "  0.60804903 0.62299192 0.47357134 0.51176741 0.26127173 0.2509105\n",
            "  0.30819427 0.68248189 0.63039173 0.87597029 0.34663212 0.53714934\n",
            "  0.45567182 0.67568548 0.19253235 0.26825283 0.18153968 0.1896374\n",
            "  0.26379957 0.18246702 0.36963314 0.88521248 0.42649181 0.57279356\n",
            "  0.92526464 0.80270456 0.29966283 0.14990757 0.33367688 0.23832207\n",
            "  0.18524974 0.91446604 0.29993915 0.79213171 0.34825023 0.4022285\n",
            "  0.58091864 0.22617003 0.31084784]]\n",
            "Proportion correct = 0.6881720430107527\n",
            "[[0.44498244 0.2898643  0.7180724  0.46060756 0.5461759  0.30763257\n",
            "  0.3222605  0.3937339  0.31464815 0.31668252 0.3566089  0.86092037\n",
            "  0.261676   0.71083    0.436616   0.5861453  0.1852951  0.31213057\n",
            "  0.7789301  0.89845645 0.4361791  0.47978333 0.16033107 0.7196305\n",
            "  0.38304317 0.40104282 0.29839462 0.26157987 0.66015136 0.29262125\n",
            "  0.4248504  0.1729719  0.24671954 0.6011821  0.1645723  0.7454375\n",
            "  0.8601208  0.17851615 0.7254495  0.5795731  0.31035405 0.74691856\n",
            "  0.63201463 0.22397977 0.27734178 0.53326076 0.3413574  0.5220477\n",
            "  0.30056095 0.26316404 0.32668188 0.25100768 0.26975507 0.19380465\n",
            "  0.608049   0.6229919  0.47357136 0.5117674  0.26127174 0.25091052\n",
            "  0.30819428 0.6824819  0.63039166 0.8759703  0.34663212 0.53714937\n",
            "  0.45567182 0.67568547 0.19253236 0.26825285 0.18153968 0.1896374\n",
            "  0.26379958 0.18246701 0.36963314 0.88521254 0.42649183 0.57279354\n",
            "  0.92526466 0.8027046  0.29966286 0.14990759 0.33367687 0.23832205\n",
            "  0.18524978 0.914466   0.29993916 0.7921318  0.34825024 0.4022285\n",
            "  0.5809186  0.22617003 0.31084782]]\n",
            "93/93 [==============================] - 0s 80us/step\n",
            "[0.6063343927424442, 0.6881720513425847]\n",
            "['loss', 'acc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9xYPRNOXJgG",
        "colab_type": "text"
      },
      "source": [
        "### 3. If you have extra time, make some exploratory plots of the data.\n",
        "\n",
        "For example code, see the notebook we looked at last class, linked to from the schedule page of the course website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6f95f7e8-9c3a-45f8-eb49-c9ebadf82fde",
        "id": "4HhbhZGsori3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "# add one line of code here to make a scatter plot\n",
        "scatter = ax.scatter(X_train[:, 2], X_train[:, 7], c = y_train[:, 0], norm = plt.Normalize(-0.2, 1.2), cmap = plt.get_cmap('plasma'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}