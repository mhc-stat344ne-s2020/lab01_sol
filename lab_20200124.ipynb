{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_20200124.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ht1ZhJT4l7",
        "colab_type": "text"
      },
      "source": [
        "# Goals\n",
        "\n",
        "The purpose of this lab is to give you a chance to practice using NumPy for manipulating arrays and making plots with matplotlib.\n",
        "\n",
        "Along the way, you'll see some of the ideas about neural networks reinforced.\n",
        "\n",
        "# Example\n",
        "\n",
        "We will work with an example of using logistic regression to predict whether someone will develop coronary heart disease (chd, our response variable) based on characteristics like their age, blood pressure, ldl cholesterol level, presence/absence of CHD in their family history, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnDTxFk02jY9",
        "colab_type": "text"
      },
      "source": [
        "# Importing Python Modules\n",
        "\n",
        "These modules extend the base functionality in Python for working with data, making plots, and fitting neural network models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvAMs0xNRa3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "np.random.seed(9533)\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import initializers\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xkCUZpp27N4",
        "colab_type": "text"
      },
      "source": [
        "# Reading in the data, initial set up\n",
        "For now, we want to concentrate on lower level calculations using NumPy; we'll talk more about using Pandas later.  That means I will do some preliminary data manipulation for us so that we can get on to today's topics, but we'll come back and talk about this kind of thing in more detail later :)  \n",
        "\n",
        "The code below reads in the data and takes a quick look at the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JE7IYojTo1E",
        "colab_type": "code",
        "outputId": "a161b3cd-643c-4bb2-bea9-eab45591a70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "saheart = pd.read_csv(\"http://www.evanlray.com/data/ESL/SAheart.data.txt\").iloc[:, 1:11]\n",
        "print(saheart.head())\n",
        "print(\"shape (rows, columns) = \" + str(saheart.shape))\n",
        "print(\"column names are: \" + str(saheart.columns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
            "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
            "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
            "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
            "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
            "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1\n",
            "shape (rows, columns) = (462, 10)\n",
            "column names are: Index(['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity',\n",
            "       'alcohol', 'age', 'chd'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWSmq7qvEVjz",
        "colab_type": "text"
      },
      "source": [
        "We need to convert famhist to a one-hot encoding, otherwise known as indicator or dummy variable.  The Pandas function `get_dummies` can do this for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8-xYw63BGSf",
        "colab_type": "code",
        "outputId": "7ec4ff9c-de97-447b-aa5d-7a5776452ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "saheart = pd.get_dummies(saheart, drop_first = True)\n",
        "print(saheart.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sbp  tobacco   ldl  adiposity  ...  alcohol  age  chd  famhist_Present\n",
            "0  160    12.00  5.73      23.11  ...    97.20   52    1                1\n",
            "1  144     0.01  4.41      28.61  ...     2.06   63    1                0\n",
            "2  118     0.08  3.48      32.28  ...     3.81   46    0                1\n",
            "3  170     7.50  6.41      38.03  ...    24.26   58    1                1\n",
            "4  134    13.60  3.50      27.78  ...    57.34   49    1                1\n",
            "\n",
            "[5 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP7OvQ3RFC_u",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to extract the data to NumPy arrays using the `to_numpy` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV-EZNCfFA-v",
        "colab_type": "code",
        "outputId": "ee597327-21ff-4103-a1f9-09b9be86d640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# X will include all columns other than the response, chd;\n",
        "# we drop that column and convert the others to numpy\n",
        "X = saheart.drop('chd', axis = 1).to_numpy()\n",
        "print(X[0:5, ])\n",
        "\n",
        "# y will include only the chd column\n",
        "y = saheart['chd'].to_numpy()\n",
        "y = y.reshape((y.shape[0], 1))\n",
        "print(y[0:5, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.600e+02 1.200e+01 5.730e+00 2.311e+01 4.900e+01 2.530e+01 9.720e+01\n",
            "  5.200e+01 1.000e+00]\n",
            " [1.440e+02 1.000e-02 4.410e+00 2.861e+01 5.500e+01 2.887e+01 2.060e+00\n",
            "  6.300e+01 0.000e+00]\n",
            " [1.180e+02 8.000e-02 3.480e+00 3.228e+01 5.200e+01 2.914e+01 3.810e+00\n",
            "  4.600e+01 1.000e+00]\n",
            " [1.700e+02 7.500e+00 6.410e+00 3.803e+01 5.100e+01 3.199e+01 2.426e+01\n",
            "  5.800e+01 1.000e+00]\n",
            " [1.340e+02 1.360e+01 3.500e+00 2.778e+01 6.000e+01 2.599e+01 5.734e+01\n",
            "  4.900e+01 1.000e+00]]\n",
            "[[1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exwJ8sV4IYQ7",
        "colab_type": "text"
      },
      "source": [
        "I will do one more step for you: a split of the data into train, validation, and test sets.  As we saw last class, model performance on the data we actually use to fit the model tends to be a little better than model performance on new data.  Basically, the model fitting process tunes the model as well as possible to the training data -- but if we look at new data from the same process we may see slightly different patterns.  We are generally interested in seeing how the model does on new data.\n",
        "\n",
        "We will use the train, validation, and test sets as follows:\n",
        " * **train**: used to actually estimate model parameters like $b$ and $w$.\n",
        " * **validation**: used to check in on how model estimation is going and possibly compare a few different candidate models\n",
        " * **test**: once we have selected our final model, we use the test data to see how well it does.\n",
        "\n",
        "Why do we need a test set in addition to a validation set?  When exploring neural networks, we may try a **lot** of different models.  That means that even though the validation set is not directly used to estimate the model parameters, we may end up choosing a model that is specifically tuned to the validation set.  We need to have a test set that was never used for any purpose at all during selecting and estimating a model, to get an honest sense of how well the model does when making predictions for data it has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ZzhDdJY8oT",
        "colab_type": "code",
        "outputId": "7730e13a-404b-4b11-f42c-beffa39fd0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(X_train[0:5, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(295, 9)\n",
            "(93, 9)\n",
            "(74, 9)\n",
            "[[160.     4.2    6.76  37.99  61.    32.91   3.09  54.     1.  ]\n",
            " [132.     7.28   3.52  12.33  60.    19.48   2.06  56.     0.  ]\n",
            " [142.     0.     3.54  16.64  58.    25.97   8.36  27.     0.  ]\n",
            " [120.     0.     3.98  13.19  47.    21.89   0.    16.     1.  ]\n",
            " [154.     0.31   2.33  16.48  33.    24.    11.83  17.     0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q6KUj_ANPJp",
        "colab_type": "text"
      },
      "source": [
        "# More data preprocessing\n",
        "\n",
        "### 1. Normalize Input Variables\n",
        "For reasons that we will discuss later, it's critical to the performance of neural networks to \"normalize\" the explanatory (input) variables.  Confusingly, in this context this does **not** mean we want them to follow a normal distribution.  What it means is that they should be standardized to have mean 0 and standard deviation 1.  In statistical terms, basically we want to give the neural network the $z$-scores of our explanatory variables rather than the original explanatory variables.\n",
        "\n",
        "To do this, you will need four lines of code that (1) calculate the column means of X_train, (2) subtract the column means from each column of X_train, (3) calculate the column standard deviations of X_train, and finally (4) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "909reb91_-oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_mean = np.mean(X_train, axis = 0) # add a call to np.mean here.  what will you use for the axis?\n",
        "X_train = X_train - X_train_mean # add code here to subtract the column means from X_train.  How will broadcasting work?\n",
        "X_train_std = np.std(X_train, axis = 0) # add a call to np.std here.  what will you use for the axis?\n",
        "X_train = X_train / X_train_std # add code here to divide X_train by the column standard deviations.  How will broadcasting work?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL2txcKZSCbl",
        "colab_type": "text"
      },
      "source": [
        "Of course, in order to apply our model to make predictions for the validation sets and test sets you will need to apply the same normalization process to X_val and X_test.  Note that we want to use exactly the same normalization for all three; you shouldn't calculate new column means, but subtract X_train_mean from X_val and X_test, and then divide X_val and X_test by X_train_std."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbaQxAtKSs0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize X_val, but using X_train_mean and X_train_std to do the normalization\n",
        "X_val = X_val - X_train_mean  # add code here to subtract the column means from X_val\n",
        "X_val = X_val / X_train_mean  # add code here to divide X_val by the column standard deviations\n",
        "\n",
        "# normalize X_test, but using X_train_mean and X_train_std to do the normalization\n",
        "X_test = X_test - X_train_mean  # add code here to subtract the column means from X_test\n",
        "X_test = X_test / X_train_mean  # add code here to divide X_test by the column standard deviations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwIfG9GFPuEc",
        "colab_type": "text"
      },
      "source": [
        "# Fitting a logistic regression model\n",
        "\n",
        "Again, we're focusing on NumPy in this lab, so I'll provide code to define and fit the model with Keras -- but you would benefit from reading through this code just so that it looks more familiar later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUOa2SbSXHs2",
        "colab_type": "code",
        "outputId": "2d5b7eaa-0a80-4594-8e5b-0336ce295532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define a logistic regression model: one layer, with a sigmoid activation and 2 inputs\n",
        "logistic_model = models.Sequential()\n",
        "logistic_model.add(layers.Dense(\n",
        "    1,\n",
        "    activation = 'sigmoid',\n",
        "    input_shape = (9,)))\n",
        "\n",
        "# compile the model using stochastic gradient descent for optimization,\n",
        "# binary cross-entropy loss, and measuring performance by classification accuracy\n",
        "#sgd = optimizers.SGD(lr=0.5, momentum=0.1, nesterov=True)\n",
        "logistic_model.compile(\n",
        "    optimizer = 'sgd',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "# Estimate the model parameters\n",
        "history = logistic_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_val, y_val),\n",
        "    epochs = 500, batch_size = X_train.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 295 samples, validate on 74 samples\n",
            "Epoch 1/500\n",
            "295/295 [==============================] - 1s 2ms/step - loss: 1.2074 - acc: 0.4034 - val_loss: 0.9062 - val_acc: 0.3649\n",
            "Epoch 2/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 1.2019 - acc: 0.4034 - val_loss: 0.9045 - val_acc: 0.3649\n",
            "Epoch 3/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 1.1964 - acc: 0.4034 - val_loss: 0.9028 - val_acc: 0.3649\n",
            "Epoch 4/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 1.1909 - acc: 0.4034 - val_loss: 0.9011 - val_acc: 0.3649\n",
            "Epoch 5/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 1.1855 - acc: 0.4034 - val_loss: 0.8994 - val_acc: 0.3649\n",
            "Epoch 6/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 1.1801 - acc: 0.4034 - val_loss: 0.8977 - val_acc: 0.3784\n",
            "Epoch 7/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 1.1747 - acc: 0.4034 - val_loss: 0.8960 - val_acc: 0.3784\n",
            "Epoch 8/500\n",
            "295/295 [==============================] - 0s 12us/step - loss: 1.1694 - acc: 0.4034 - val_loss: 0.8943 - val_acc: 0.3784\n",
            "Epoch 9/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 1.1641 - acc: 0.4068 - val_loss: 0.8927 - val_acc: 0.3784\n",
            "Epoch 10/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 1.1588 - acc: 0.4068 - val_loss: 0.8910 - val_acc: 0.3784\n",
            "Epoch 11/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 1.1536 - acc: 0.4068 - val_loss: 0.8894 - val_acc: 0.3784\n",
            "Epoch 12/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 1.1484 - acc: 0.4068 - val_loss: 0.8878 - val_acc: 0.3784\n",
            "Epoch 13/500\n",
            "295/295 [==============================] - 0s 36us/step - loss: 1.1432 - acc: 0.4068 - val_loss: 0.8861 - val_acc: 0.3784\n",
            "Epoch 14/500\n",
            "295/295 [==============================] - 0s 38us/step - loss: 1.1381 - acc: 0.4068 - val_loss: 0.8845 - val_acc: 0.3784\n",
            "Epoch 15/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 1.1330 - acc: 0.4102 - val_loss: 0.8829 - val_acc: 0.3784\n",
            "Epoch 16/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 1.1279 - acc: 0.4102 - val_loss: 0.8814 - val_acc: 0.3784\n",
            "Epoch 17/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 1.1229 - acc: 0.4102 - val_loss: 0.8798 - val_acc: 0.3784\n",
            "Epoch 18/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 1.1179 - acc: 0.4102 - val_loss: 0.8782 - val_acc: 0.3784\n",
            "Epoch 19/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 1.1129 - acc: 0.4102 - val_loss: 0.8767 - val_acc: 0.3919\n",
            "Epoch 20/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 1.1080 - acc: 0.4102 - val_loss: 0.8751 - val_acc: 0.3919\n",
            "Epoch 21/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 1.1031 - acc: 0.4102 - val_loss: 0.8736 - val_acc: 0.3919\n",
            "Epoch 22/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 1.0982 - acc: 0.4102 - val_loss: 0.8721 - val_acc: 0.4054\n",
            "Epoch 23/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 1.0934 - acc: 0.4102 - val_loss: 0.8706 - val_acc: 0.4054\n",
            "Epoch 24/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 1.0886 - acc: 0.4102 - val_loss: 0.8690 - val_acc: 0.4054\n",
            "Epoch 25/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 1.0838 - acc: 0.4136 - val_loss: 0.8676 - val_acc: 0.4054\n",
            "Epoch 26/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 1.0791 - acc: 0.4136 - val_loss: 0.8661 - val_acc: 0.4054\n",
            "Epoch 27/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 1.0744 - acc: 0.4169 - val_loss: 0.8646 - val_acc: 0.4054\n",
            "Epoch 28/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 1.0698 - acc: 0.4237 - val_loss: 0.8631 - val_acc: 0.4054\n",
            "Epoch 29/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 1.0652 - acc: 0.4237 - val_loss: 0.8617 - val_acc: 0.4054\n",
            "Epoch 30/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 1.0606 - acc: 0.4237 - val_loss: 0.8602 - val_acc: 0.4054\n",
            "Epoch 31/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 1.0560 - acc: 0.4305 - val_loss: 0.8588 - val_acc: 0.4054\n",
            "Epoch 32/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 1.0515 - acc: 0.4305 - val_loss: 0.8574 - val_acc: 0.4054\n",
            "Epoch 33/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 1.0470 - acc: 0.4305 - val_loss: 0.8560 - val_acc: 0.4054\n",
            "Epoch 34/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 1.0425 - acc: 0.4305 - val_loss: 0.8546 - val_acc: 0.4054\n",
            "Epoch 35/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 1.0381 - acc: 0.4339 - val_loss: 0.8532 - val_acc: 0.4054\n",
            "Epoch 36/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 1.0337 - acc: 0.4339 - val_loss: 0.8518 - val_acc: 0.4054\n",
            "Epoch 37/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 1.0294 - acc: 0.4373 - val_loss: 0.8504 - val_acc: 0.4054\n",
            "Epoch 38/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 1.0251 - acc: 0.4407 - val_loss: 0.8490 - val_acc: 0.4054\n",
            "Epoch 39/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 1.0208 - acc: 0.4407 - val_loss: 0.8477 - val_acc: 0.4054\n",
            "Epoch 40/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 1.0165 - acc: 0.4407 - val_loss: 0.8463 - val_acc: 0.4054\n",
            "Epoch 41/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 1.0123 - acc: 0.4407 - val_loss: 0.8450 - val_acc: 0.4189\n",
            "Epoch 42/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 1.0081 - acc: 0.4407 - val_loss: 0.8437 - val_acc: 0.4189\n",
            "Epoch 43/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 1.0040 - acc: 0.4441 - val_loss: 0.8423 - val_acc: 0.4189\n",
            "Epoch 44/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.9998 - acc: 0.4441 - val_loss: 0.8410 - val_acc: 0.4324\n",
            "Epoch 45/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.9957 - acc: 0.4441 - val_loss: 0.8397 - val_acc: 0.4324\n",
            "Epoch 46/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.9917 - acc: 0.4441 - val_loss: 0.8384 - val_acc: 0.4324\n",
            "Epoch 47/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.9877 - acc: 0.4441 - val_loss: 0.8371 - val_acc: 0.4324\n",
            "Epoch 48/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.9837 - acc: 0.4441 - val_loss: 0.8359 - val_acc: 0.4324\n",
            "Epoch 49/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.9797 - acc: 0.4475 - val_loss: 0.8346 - val_acc: 0.4459\n",
            "Epoch 50/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.9758 - acc: 0.4475 - val_loss: 0.8333 - val_acc: 0.4459\n",
            "Epoch 51/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.9719 - acc: 0.4475 - val_loss: 0.8321 - val_acc: 0.4459\n",
            "Epoch 52/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.9680 - acc: 0.4441 - val_loss: 0.8308 - val_acc: 0.4595\n",
            "Epoch 53/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.9642 - acc: 0.4475 - val_loss: 0.8296 - val_acc: 0.4595\n",
            "Epoch 54/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.9604 - acc: 0.4508 - val_loss: 0.8284 - val_acc: 0.4595\n",
            "Epoch 55/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.9566 - acc: 0.4508 - val_loss: 0.8272 - val_acc: 0.4595\n",
            "Epoch 56/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.9529 - acc: 0.4508 - val_loss: 0.8260 - val_acc: 0.4595\n",
            "Epoch 57/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.9492 - acc: 0.4508 - val_loss: 0.8248 - val_acc: 0.4595\n",
            "Epoch 58/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.9455 - acc: 0.4542 - val_loss: 0.8236 - val_acc: 0.4595\n",
            "Epoch 59/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.9419 - acc: 0.4576 - val_loss: 0.8224 - val_acc: 0.4595\n",
            "Epoch 60/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.9383 - acc: 0.4542 - val_loss: 0.8212 - val_acc: 0.4595\n",
            "Epoch 61/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.9347 - acc: 0.4610 - val_loss: 0.8201 - val_acc: 0.4595\n",
            "Epoch 62/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.9311 - acc: 0.4678 - val_loss: 0.8189 - val_acc: 0.4595\n",
            "Epoch 63/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.9276 - acc: 0.4712 - val_loss: 0.8178 - val_acc: 0.4595\n",
            "Epoch 64/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.9241 - acc: 0.4712 - val_loss: 0.8166 - val_acc: 0.4595\n",
            "Epoch 65/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.9207 - acc: 0.4746 - val_loss: 0.8155 - val_acc: 0.4730\n",
            "Epoch 66/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.9172 - acc: 0.4746 - val_loss: 0.8144 - val_acc: 0.4730\n",
            "Epoch 67/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.9138 - acc: 0.4712 - val_loss: 0.8132 - val_acc: 0.4730\n",
            "Epoch 68/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.9105 - acc: 0.4712 - val_loss: 0.8121 - val_acc: 0.4730\n",
            "Epoch 69/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.9071 - acc: 0.4712 - val_loss: 0.8110 - val_acc: 0.4730\n",
            "Epoch 70/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.9038 - acc: 0.4712 - val_loss: 0.8099 - val_acc: 0.4865\n",
            "Epoch 71/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.9005 - acc: 0.4712 - val_loss: 0.8089 - val_acc: 0.4865\n",
            "Epoch 72/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.8973 - acc: 0.4678 - val_loss: 0.8078 - val_acc: 0.4865\n",
            "Epoch 73/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.8941 - acc: 0.4712 - val_loss: 0.8067 - val_acc: 0.4865\n",
            "Epoch 74/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.8909 - acc: 0.4712 - val_loss: 0.8056 - val_acc: 0.5000\n",
            "Epoch 75/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.8877 - acc: 0.4712 - val_loss: 0.8046 - val_acc: 0.4865\n",
            "Epoch 76/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.8846 - acc: 0.4712 - val_loss: 0.8035 - val_acc: 0.4865\n",
            "Epoch 77/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.8815 - acc: 0.4746 - val_loss: 0.8025 - val_acc: 0.5000\n",
            "Epoch 78/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.8784 - acc: 0.4746 - val_loss: 0.8015 - val_acc: 0.5000\n",
            "Epoch 79/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.8754 - acc: 0.4746 - val_loss: 0.8004 - val_acc: 0.5270\n",
            "Epoch 80/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.8723 - acc: 0.4780 - val_loss: 0.7994 - val_acc: 0.5405\n",
            "Epoch 81/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8693 - acc: 0.4814 - val_loss: 0.7984 - val_acc: 0.5405\n",
            "Epoch 82/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.8664 - acc: 0.4780 - val_loss: 0.7974 - val_acc: 0.5270\n",
            "Epoch 83/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8634 - acc: 0.4814 - val_loss: 0.7964 - val_acc: 0.5270\n",
            "Epoch 84/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.8605 - acc: 0.4814 - val_loss: 0.7954 - val_acc: 0.5270\n",
            "Epoch 85/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.8576 - acc: 0.4814 - val_loss: 0.7944 - val_acc: 0.5270\n",
            "Epoch 86/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.8548 - acc: 0.4814 - val_loss: 0.7935 - val_acc: 0.5405\n",
            "Epoch 87/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8520 - acc: 0.4881 - val_loss: 0.7925 - val_acc: 0.5541\n",
            "Epoch 88/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.8492 - acc: 0.4881 - val_loss: 0.7915 - val_acc: 0.5541\n",
            "Epoch 89/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.8464 - acc: 0.4915 - val_loss: 0.7906 - val_acc: 0.5676\n",
            "Epoch 90/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.8436 - acc: 0.4915 - val_loss: 0.7896 - val_acc: 0.5676\n",
            "Epoch 91/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.8409 - acc: 0.4949 - val_loss: 0.7887 - val_acc: 0.5676\n",
            "Epoch 92/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.8382 - acc: 0.4949 - val_loss: 0.7877 - val_acc: 0.5676\n",
            "Epoch 93/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.8355 - acc: 0.4949 - val_loss: 0.7868 - val_acc: 0.5676\n",
            "Epoch 94/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.8329 - acc: 0.4915 - val_loss: 0.7859 - val_acc: 0.5676\n",
            "Epoch 95/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.8303 - acc: 0.4983 - val_loss: 0.7850 - val_acc: 0.5676\n",
            "Epoch 96/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.8277 - acc: 0.5017 - val_loss: 0.7841 - val_acc: 0.5676\n",
            "Epoch 97/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.8251 - acc: 0.5153 - val_loss: 0.7831 - val_acc: 0.5811\n",
            "Epoch 98/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.8225 - acc: 0.5153 - val_loss: 0.7822 - val_acc: 0.5811\n",
            "Epoch 99/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.8200 - acc: 0.5153 - val_loss: 0.7814 - val_acc: 0.5811\n",
            "Epoch 100/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.8175 - acc: 0.5153 - val_loss: 0.7805 - val_acc: 0.5811\n",
            "Epoch 101/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.8150 - acc: 0.5153 - val_loss: 0.7796 - val_acc: 0.5811\n",
            "Epoch 102/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.8126 - acc: 0.5153 - val_loss: 0.7787 - val_acc: 0.5946\n",
            "Epoch 103/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.8102 - acc: 0.5153 - val_loss: 0.7778 - val_acc: 0.5946\n",
            "Epoch 104/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.8077 - acc: 0.5153 - val_loss: 0.7770 - val_acc: 0.5946\n",
            "Epoch 105/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.8054 - acc: 0.5119 - val_loss: 0.7761 - val_acc: 0.5946\n",
            "Epoch 106/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.8030 - acc: 0.5153 - val_loss: 0.7753 - val_acc: 0.5946\n",
            "Epoch 107/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.8007 - acc: 0.5153 - val_loss: 0.7744 - val_acc: 0.6081\n",
            "Epoch 108/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7984 - acc: 0.5153 - val_loss: 0.7736 - val_acc: 0.6081\n",
            "Epoch 109/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7961 - acc: 0.5153 - val_loss: 0.7728 - val_acc: 0.6081\n",
            "Epoch 110/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7938 - acc: 0.5186 - val_loss: 0.7719 - val_acc: 0.6081\n",
            "Epoch 111/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7916 - acc: 0.5220 - val_loss: 0.7711 - val_acc: 0.6081\n",
            "Epoch 112/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7893 - acc: 0.5254 - val_loss: 0.7703 - val_acc: 0.6081\n",
            "Epoch 113/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7871 - acc: 0.5254 - val_loss: 0.7695 - val_acc: 0.6081\n",
            "Epoch 114/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7850 - acc: 0.5288 - val_loss: 0.7687 - val_acc: 0.6216\n",
            "Epoch 115/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7828 - acc: 0.5356 - val_loss: 0.7679 - val_acc: 0.6216\n",
            "Epoch 116/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.7807 - acc: 0.5356 - val_loss: 0.7671 - val_acc: 0.6216\n",
            "Epoch 117/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7785 - acc: 0.5390 - val_loss: 0.7663 - val_acc: 0.6216\n",
            "Epoch 118/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.7765 - acc: 0.5390 - val_loss: 0.7655 - val_acc: 0.6216\n",
            "Epoch 119/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.7744 - acc: 0.5390 - val_loss: 0.7647 - val_acc: 0.6216\n",
            "Epoch 120/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.7723 - acc: 0.5390 - val_loss: 0.7640 - val_acc: 0.6216\n",
            "Epoch 121/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.7703 - acc: 0.5424 - val_loss: 0.7632 - val_acc: 0.6216\n",
            "Epoch 122/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.7683 - acc: 0.5424 - val_loss: 0.7624 - val_acc: 0.6216\n",
            "Epoch 123/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.7663 - acc: 0.5424 - val_loss: 0.7617 - val_acc: 0.6216\n",
            "Epoch 124/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.7643 - acc: 0.5424 - val_loss: 0.7609 - val_acc: 0.6216\n",
            "Epoch 125/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7624 - acc: 0.5424 - val_loss: 0.7602 - val_acc: 0.6216\n",
            "Epoch 126/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.7604 - acc: 0.5424 - val_loss: 0.7594 - val_acc: 0.6216\n",
            "Epoch 127/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.7585 - acc: 0.5458 - val_loss: 0.7587 - val_acc: 0.6216\n",
            "Epoch 128/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7566 - acc: 0.5458 - val_loss: 0.7579 - val_acc: 0.6216\n",
            "Epoch 129/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.7547 - acc: 0.5458 - val_loss: 0.7572 - val_acc: 0.6216\n",
            "Epoch 130/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.7529 - acc: 0.5492 - val_loss: 0.7565 - val_acc: 0.6216\n",
            "Epoch 131/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7510 - acc: 0.5559 - val_loss: 0.7558 - val_acc: 0.6216\n",
            "Epoch 132/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.7492 - acc: 0.5559 - val_loss: 0.7551 - val_acc: 0.6216\n",
            "Epoch 133/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.7474 - acc: 0.5559 - val_loss: 0.7543 - val_acc: 0.6216\n",
            "Epoch 134/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7456 - acc: 0.5559 - val_loss: 0.7536 - val_acc: 0.6216\n",
            "Epoch 135/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7439 - acc: 0.5525 - val_loss: 0.7529 - val_acc: 0.6216\n",
            "Epoch 136/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.7421 - acc: 0.5559 - val_loss: 0.7522 - val_acc: 0.6216\n",
            "Epoch 137/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.7404 - acc: 0.5593 - val_loss: 0.7515 - val_acc: 0.6216\n",
            "Epoch 138/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7387 - acc: 0.5593 - val_loss: 0.7509 - val_acc: 0.6216\n",
            "Epoch 139/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7370 - acc: 0.5627 - val_loss: 0.7502 - val_acc: 0.6216\n",
            "Epoch 140/500\n",
            "295/295 [==============================] - 0s 40us/step - loss: 0.7353 - acc: 0.5627 - val_loss: 0.7495 - val_acc: 0.6216\n",
            "Epoch 141/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.7336 - acc: 0.5627 - val_loss: 0.7488 - val_acc: 0.6216\n",
            "Epoch 142/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.7320 - acc: 0.5661 - val_loss: 0.7481 - val_acc: 0.6216\n",
            "Epoch 143/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7304 - acc: 0.5627 - val_loss: 0.7475 - val_acc: 0.6216\n",
            "Epoch 144/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7287 - acc: 0.5627 - val_loss: 0.7468 - val_acc: 0.6216\n",
            "Epoch 145/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7271 - acc: 0.5661 - val_loss: 0.7462 - val_acc: 0.6216\n",
            "Epoch 146/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7256 - acc: 0.5695 - val_loss: 0.7455 - val_acc: 0.6216\n",
            "Epoch 147/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7240 - acc: 0.5729 - val_loss: 0.7449 - val_acc: 0.6216\n",
            "Epoch 148/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7224 - acc: 0.5695 - val_loss: 0.7442 - val_acc: 0.6216\n",
            "Epoch 149/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7209 - acc: 0.5695 - val_loss: 0.7436 - val_acc: 0.6216\n",
            "Epoch 150/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7194 - acc: 0.5695 - val_loss: 0.7429 - val_acc: 0.6216\n",
            "Epoch 151/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7179 - acc: 0.5729 - val_loss: 0.7423 - val_acc: 0.6216\n",
            "Epoch 152/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7164 - acc: 0.5729 - val_loss: 0.7417 - val_acc: 0.6351\n",
            "Epoch 153/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.7149 - acc: 0.5763 - val_loss: 0.7410 - val_acc: 0.6351\n",
            "Epoch 154/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.7135 - acc: 0.5763 - val_loss: 0.7404 - val_acc: 0.6351\n",
            "Epoch 155/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.7120 - acc: 0.5763 - val_loss: 0.7398 - val_acc: 0.6351\n",
            "Epoch 156/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7106 - acc: 0.5797 - val_loss: 0.7392 - val_acc: 0.6351\n",
            "Epoch 157/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7092 - acc: 0.5831 - val_loss: 0.7386 - val_acc: 0.6351\n",
            "Epoch 158/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.7078 - acc: 0.5831 - val_loss: 0.7380 - val_acc: 0.6351\n",
            "Epoch 159/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7064 - acc: 0.5864 - val_loss: 0.7374 - val_acc: 0.6351\n",
            "Epoch 160/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.7050 - acc: 0.5864 - val_loss: 0.7368 - val_acc: 0.6351\n",
            "Epoch 161/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.7037 - acc: 0.5864 - val_loss: 0.7362 - val_acc: 0.6351\n",
            "Epoch 162/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.7023 - acc: 0.5864 - val_loss: 0.7356 - val_acc: 0.6351\n",
            "Epoch 163/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.7010 - acc: 0.5898 - val_loss: 0.7350 - val_acc: 0.6351\n",
            "Epoch 164/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6997 - acc: 0.5932 - val_loss: 0.7344 - val_acc: 0.6351\n",
            "Epoch 165/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6984 - acc: 0.5932 - val_loss: 0.7338 - val_acc: 0.6351\n",
            "Epoch 166/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6971 - acc: 0.5932 - val_loss: 0.7332 - val_acc: 0.6351\n",
            "Epoch 167/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6958 - acc: 0.5932 - val_loss: 0.7327 - val_acc: 0.6351\n",
            "Epoch 168/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6945 - acc: 0.5932 - val_loss: 0.7321 - val_acc: 0.6351\n",
            "Epoch 169/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6933 - acc: 0.5966 - val_loss: 0.7315 - val_acc: 0.6351\n",
            "Epoch 170/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6920 - acc: 0.5966 - val_loss: 0.7309 - val_acc: 0.6351\n",
            "Epoch 171/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6908 - acc: 0.6000 - val_loss: 0.7304 - val_acc: 0.6351\n",
            "Epoch 172/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6896 - acc: 0.6000 - val_loss: 0.7298 - val_acc: 0.6351\n",
            "Epoch 173/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6884 - acc: 0.6000 - val_loss: 0.7293 - val_acc: 0.6351\n",
            "Epoch 174/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6872 - acc: 0.5966 - val_loss: 0.7287 - val_acc: 0.6351\n",
            "Epoch 175/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6860 - acc: 0.5966 - val_loss: 0.7282 - val_acc: 0.6351\n",
            "Epoch 176/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6848 - acc: 0.6000 - val_loss: 0.7276 - val_acc: 0.6351\n",
            "Epoch 177/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6836 - acc: 0.6000 - val_loss: 0.7271 - val_acc: 0.6351\n",
            "Epoch 178/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6825 - acc: 0.6034 - val_loss: 0.7265 - val_acc: 0.6351\n",
            "Epoch 179/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6814 - acc: 0.6068 - val_loss: 0.7260 - val_acc: 0.6351\n",
            "Epoch 180/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6802 - acc: 0.6068 - val_loss: 0.7255 - val_acc: 0.6351\n",
            "Epoch 181/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6791 - acc: 0.6102 - val_loss: 0.7249 - val_acc: 0.6351\n",
            "Epoch 182/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6780 - acc: 0.6102 - val_loss: 0.7244 - val_acc: 0.6351\n",
            "Epoch 183/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6769 - acc: 0.6102 - val_loss: 0.7239 - val_acc: 0.6351\n",
            "Epoch 184/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6758 - acc: 0.6102 - val_loss: 0.7233 - val_acc: 0.6351\n",
            "Epoch 185/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6748 - acc: 0.6102 - val_loss: 0.7228 - val_acc: 0.6216\n",
            "Epoch 186/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6737 - acc: 0.6203 - val_loss: 0.7223 - val_acc: 0.6216\n",
            "Epoch 187/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6727 - acc: 0.6203 - val_loss: 0.7218 - val_acc: 0.6216\n",
            "Epoch 188/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6716 - acc: 0.6237 - val_loss: 0.7213 - val_acc: 0.6216\n",
            "Epoch 189/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6706 - acc: 0.6237 - val_loss: 0.7208 - val_acc: 0.6216\n",
            "Epoch 190/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6696 - acc: 0.6237 - val_loss: 0.7203 - val_acc: 0.6216\n",
            "Epoch 191/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6686 - acc: 0.6237 - val_loss: 0.7198 - val_acc: 0.6216\n",
            "Epoch 192/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6676 - acc: 0.6237 - val_loss: 0.7193 - val_acc: 0.6216\n",
            "Epoch 193/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6666 - acc: 0.6271 - val_loss: 0.7188 - val_acc: 0.6216\n",
            "Epoch 194/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6656 - acc: 0.6271 - val_loss: 0.7183 - val_acc: 0.6216\n",
            "Epoch 195/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6646 - acc: 0.6271 - val_loss: 0.7178 - val_acc: 0.6216\n",
            "Epoch 196/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6637 - acc: 0.6271 - val_loss: 0.7173 - val_acc: 0.6216\n",
            "Epoch 197/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6627 - acc: 0.6271 - val_loss: 0.7168 - val_acc: 0.6216\n",
            "Epoch 198/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6618 - acc: 0.6305 - val_loss: 0.7163 - val_acc: 0.6216\n",
            "Epoch 199/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.6608 - acc: 0.6305 - val_loss: 0.7158 - val_acc: 0.6216\n",
            "Epoch 200/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6599 - acc: 0.6339 - val_loss: 0.7154 - val_acc: 0.6216\n",
            "Epoch 201/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6590 - acc: 0.6407 - val_loss: 0.7149 - val_acc: 0.6216\n",
            "Epoch 202/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6581 - acc: 0.6441 - val_loss: 0.7144 - val_acc: 0.6216\n",
            "Epoch 203/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6572 - acc: 0.6441 - val_loss: 0.7139 - val_acc: 0.6216\n",
            "Epoch 204/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6563 - acc: 0.6475 - val_loss: 0.7135 - val_acc: 0.6216\n",
            "Epoch 205/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6554 - acc: 0.6475 - val_loss: 0.7130 - val_acc: 0.6351\n",
            "Epoch 206/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6545 - acc: 0.6475 - val_loss: 0.7125 - val_acc: 0.6351\n",
            "Epoch 207/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6537 - acc: 0.6475 - val_loss: 0.7121 - val_acc: 0.6351\n",
            "Epoch 208/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6528 - acc: 0.6475 - val_loss: 0.7116 - val_acc: 0.6351\n",
            "Epoch 209/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6520 - acc: 0.6475 - val_loss: 0.7111 - val_acc: 0.6351\n",
            "Epoch 210/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6511 - acc: 0.6475 - val_loss: 0.7107 - val_acc: 0.6351\n",
            "Epoch 211/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.6503 - acc: 0.6475 - val_loss: 0.7102 - val_acc: 0.6351\n",
            "Epoch 212/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6495 - acc: 0.6508 - val_loss: 0.7098 - val_acc: 0.6351\n",
            "Epoch 213/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6486 - acc: 0.6542 - val_loss: 0.7093 - val_acc: 0.6351\n",
            "Epoch 214/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6478 - acc: 0.6542 - val_loss: 0.7089 - val_acc: 0.6351\n",
            "Epoch 215/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6470 - acc: 0.6542 - val_loss: 0.7084 - val_acc: 0.6351\n",
            "Epoch 216/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6462 - acc: 0.6542 - val_loss: 0.7080 - val_acc: 0.6351\n",
            "Epoch 217/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6454 - acc: 0.6542 - val_loss: 0.7076 - val_acc: 0.6351\n",
            "Epoch 218/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6447 - acc: 0.6542 - val_loss: 0.7071 - val_acc: 0.6351\n",
            "Epoch 219/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6439 - acc: 0.6576 - val_loss: 0.7067 - val_acc: 0.6351\n",
            "Epoch 220/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6431 - acc: 0.6576 - val_loss: 0.7063 - val_acc: 0.6351\n",
            "Epoch 221/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6424 - acc: 0.6576 - val_loss: 0.7058 - val_acc: 0.6351\n",
            "Epoch 222/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6416 - acc: 0.6576 - val_loss: 0.7054 - val_acc: 0.6351\n",
            "Epoch 223/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6409 - acc: 0.6576 - val_loss: 0.7050 - val_acc: 0.6351\n",
            "Epoch 224/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6401 - acc: 0.6542 - val_loss: 0.7045 - val_acc: 0.6351\n",
            "Epoch 225/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6394 - acc: 0.6576 - val_loss: 0.7041 - val_acc: 0.6351\n",
            "Epoch 226/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6387 - acc: 0.6542 - val_loss: 0.7037 - val_acc: 0.6351\n",
            "Epoch 227/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6380 - acc: 0.6576 - val_loss: 0.7033 - val_acc: 0.6351\n",
            "Epoch 228/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6372 - acc: 0.6610 - val_loss: 0.7029 - val_acc: 0.6351\n",
            "Epoch 229/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6365 - acc: 0.6644 - val_loss: 0.7025 - val_acc: 0.6351\n",
            "Epoch 230/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6358 - acc: 0.6644 - val_loss: 0.7020 - val_acc: 0.6351\n",
            "Epoch 231/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6352 - acc: 0.6644 - val_loss: 0.7016 - val_acc: 0.6351\n",
            "Epoch 232/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6345 - acc: 0.6746 - val_loss: 0.7012 - val_acc: 0.6351\n",
            "Epoch 233/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6338 - acc: 0.6746 - val_loss: 0.7008 - val_acc: 0.6351\n",
            "Epoch 234/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6331 - acc: 0.6746 - val_loss: 0.7004 - val_acc: 0.6351\n",
            "Epoch 235/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6324 - acc: 0.6780 - val_loss: 0.7000 - val_acc: 0.6351\n",
            "Epoch 236/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6318 - acc: 0.6780 - val_loss: 0.6996 - val_acc: 0.6351\n",
            "Epoch 237/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6311 - acc: 0.6814 - val_loss: 0.6992 - val_acc: 0.6351\n",
            "Epoch 238/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6305 - acc: 0.6814 - val_loss: 0.6988 - val_acc: 0.6351\n",
            "Epoch 239/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6298 - acc: 0.6814 - val_loss: 0.6984 - val_acc: 0.6486\n",
            "Epoch 240/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6292 - acc: 0.6814 - val_loss: 0.6980 - val_acc: 0.6486\n",
            "Epoch 241/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6286 - acc: 0.6814 - val_loss: 0.6976 - val_acc: 0.6486\n",
            "Epoch 242/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6279 - acc: 0.6814 - val_loss: 0.6972 - val_acc: 0.6486\n",
            "Epoch 243/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6273 - acc: 0.6814 - val_loss: 0.6968 - val_acc: 0.6486\n",
            "Epoch 244/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6267 - acc: 0.6814 - val_loss: 0.6965 - val_acc: 0.6486\n",
            "Epoch 245/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.6261 - acc: 0.6814 - val_loss: 0.6961 - val_acc: 0.6486\n",
            "Epoch 246/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6255 - acc: 0.6814 - val_loss: 0.6957 - val_acc: 0.6486\n",
            "Epoch 247/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6249 - acc: 0.6814 - val_loss: 0.6953 - val_acc: 0.6486\n",
            "Epoch 248/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6243 - acc: 0.6847 - val_loss: 0.6949 - val_acc: 0.6486\n",
            "Epoch 249/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6237 - acc: 0.6847 - val_loss: 0.6945 - val_acc: 0.6486\n",
            "Epoch 250/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6231 - acc: 0.6881 - val_loss: 0.6942 - val_acc: 0.6486\n",
            "Epoch 251/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6225 - acc: 0.6881 - val_loss: 0.6938 - val_acc: 0.6486\n",
            "Epoch 252/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6220 - acc: 0.6881 - val_loss: 0.6934 - val_acc: 0.6486\n",
            "Epoch 253/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6214 - acc: 0.6881 - val_loss: 0.6930 - val_acc: 0.6486\n",
            "Epoch 254/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6208 - acc: 0.6881 - val_loss: 0.6927 - val_acc: 0.6486\n",
            "Epoch 255/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6203 - acc: 0.6881 - val_loss: 0.6923 - val_acc: 0.6486\n",
            "Epoch 256/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6197 - acc: 0.6881 - val_loss: 0.6919 - val_acc: 0.6486\n",
            "Epoch 257/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6192 - acc: 0.6881 - val_loss: 0.6916 - val_acc: 0.6486\n",
            "Epoch 258/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6186 - acc: 0.6915 - val_loss: 0.6912 - val_acc: 0.6486\n",
            "Epoch 259/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6181 - acc: 0.6915 - val_loss: 0.6908 - val_acc: 0.6486\n",
            "Epoch 260/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6176 - acc: 0.6915 - val_loss: 0.6905 - val_acc: 0.6486\n",
            "Epoch 261/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6170 - acc: 0.6915 - val_loss: 0.6901 - val_acc: 0.6486\n",
            "Epoch 262/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6165 - acc: 0.6915 - val_loss: 0.6898 - val_acc: 0.6486\n",
            "Epoch 263/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.6160 - acc: 0.6915 - val_loss: 0.6894 - val_acc: 0.6486\n",
            "Epoch 264/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6155 - acc: 0.6881 - val_loss: 0.6891 - val_acc: 0.6486\n",
            "Epoch 265/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6150 - acc: 0.6881 - val_loss: 0.6887 - val_acc: 0.6486\n",
            "Epoch 266/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6144 - acc: 0.6881 - val_loss: 0.6884 - val_acc: 0.6486\n",
            "Epoch 267/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6139 - acc: 0.6814 - val_loss: 0.6880 - val_acc: 0.6486\n",
            "Epoch 268/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6134 - acc: 0.6814 - val_loss: 0.6877 - val_acc: 0.6486\n",
            "Epoch 269/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.6129 - acc: 0.6814 - val_loss: 0.6873 - val_acc: 0.6486\n",
            "Epoch 270/500\n",
            "295/295 [==============================] - 0s 44us/step - loss: 0.6125 - acc: 0.6814 - val_loss: 0.6870 - val_acc: 0.6486\n",
            "Epoch 271/500\n",
            "295/295 [==============================] - 0s 36us/step - loss: 0.6120 - acc: 0.6814 - val_loss: 0.6866 - val_acc: 0.6486\n",
            "Epoch 272/500\n",
            "295/295 [==============================] - 0s 33us/step - loss: 0.6115 - acc: 0.6780 - val_loss: 0.6863 - val_acc: 0.6486\n",
            "Epoch 273/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.6110 - acc: 0.6780 - val_loss: 0.6859 - val_acc: 0.6486\n",
            "Epoch 274/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6105 - acc: 0.6814 - val_loss: 0.6856 - val_acc: 0.6351\n",
            "Epoch 275/500\n",
            "295/295 [==============================] - 0s 35us/step - loss: 0.6101 - acc: 0.6814 - val_loss: 0.6852 - val_acc: 0.6351\n",
            "Epoch 276/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.6096 - acc: 0.6814 - val_loss: 0.6849 - val_acc: 0.6351\n",
            "Epoch 277/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6091 - acc: 0.6814 - val_loss: 0.6846 - val_acc: 0.6351\n",
            "Epoch 278/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.6087 - acc: 0.6814 - val_loss: 0.6842 - val_acc: 0.6351\n",
            "Epoch 279/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6082 - acc: 0.6780 - val_loss: 0.6839 - val_acc: 0.6351\n",
            "Epoch 280/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6078 - acc: 0.6780 - val_loss: 0.6836 - val_acc: 0.6351\n",
            "Epoch 281/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6073 - acc: 0.6780 - val_loss: 0.6832 - val_acc: 0.6351\n",
            "Epoch 282/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.6069 - acc: 0.6780 - val_loss: 0.6829 - val_acc: 0.6351\n",
            "Epoch 283/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6064 - acc: 0.6780 - val_loss: 0.6826 - val_acc: 0.6351\n",
            "Epoch 284/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.6060 - acc: 0.6780 - val_loss: 0.6823 - val_acc: 0.6351\n",
            "Epoch 285/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6056 - acc: 0.6780 - val_loss: 0.6819 - val_acc: 0.6351\n",
            "Epoch 286/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6051 - acc: 0.6780 - val_loss: 0.6816 - val_acc: 0.6351\n",
            "Epoch 287/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6047 - acc: 0.6780 - val_loss: 0.6813 - val_acc: 0.6351\n",
            "Epoch 288/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6043 - acc: 0.6780 - val_loss: 0.6810 - val_acc: 0.6351\n",
            "Epoch 289/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6038 - acc: 0.6780 - val_loss: 0.6806 - val_acc: 0.6351\n",
            "Epoch 290/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6034 - acc: 0.6780 - val_loss: 0.6803 - val_acc: 0.6351\n",
            "Epoch 291/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.6030 - acc: 0.6780 - val_loss: 0.6800 - val_acc: 0.6351\n",
            "Epoch 292/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.6026 - acc: 0.6780 - val_loss: 0.6797 - val_acc: 0.6351\n",
            "Epoch 293/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.6022 - acc: 0.6746 - val_loss: 0.6794 - val_acc: 0.6351\n",
            "Epoch 294/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6018 - acc: 0.6746 - val_loss: 0.6791 - val_acc: 0.6351\n",
            "Epoch 295/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.6014 - acc: 0.6746 - val_loss: 0.6787 - val_acc: 0.6351\n",
            "Epoch 296/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.6010 - acc: 0.6746 - val_loss: 0.6784 - val_acc: 0.6351\n",
            "Epoch 297/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.6006 - acc: 0.6780 - val_loss: 0.6781 - val_acc: 0.6351\n",
            "Epoch 298/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.6002 - acc: 0.6780 - val_loss: 0.6778 - val_acc: 0.6351\n",
            "Epoch 299/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5998 - acc: 0.6780 - val_loss: 0.6775 - val_acc: 0.6351\n",
            "Epoch 300/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5994 - acc: 0.6746 - val_loss: 0.6772 - val_acc: 0.6351\n",
            "Epoch 301/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.5991 - acc: 0.6746 - val_loss: 0.6769 - val_acc: 0.6351\n",
            "Epoch 302/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5987 - acc: 0.6780 - val_loss: 0.6766 - val_acc: 0.6351\n",
            "Epoch 303/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5983 - acc: 0.6780 - val_loss: 0.6763 - val_acc: 0.6351\n",
            "Epoch 304/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5979 - acc: 0.6780 - val_loss: 0.6760 - val_acc: 0.6351\n",
            "Epoch 305/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5976 - acc: 0.6780 - val_loss: 0.6757 - val_acc: 0.6351\n",
            "Epoch 306/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5972 - acc: 0.6780 - val_loss: 0.6754 - val_acc: 0.6351\n",
            "Epoch 307/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5968 - acc: 0.6814 - val_loss: 0.6751 - val_acc: 0.6351\n",
            "Epoch 308/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5965 - acc: 0.6814 - val_loss: 0.6748 - val_acc: 0.6486\n",
            "Epoch 309/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5961 - acc: 0.6814 - val_loss: 0.6745 - val_acc: 0.6486\n",
            "Epoch 310/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5957 - acc: 0.6814 - val_loss: 0.6742 - val_acc: 0.6486\n",
            "Epoch 311/500\n",
            "295/295 [==============================] - 0s 13us/step - loss: 0.5954 - acc: 0.6814 - val_loss: 0.6739 - val_acc: 0.6486\n",
            "Epoch 312/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5950 - acc: 0.6780 - val_loss: 0.6736 - val_acc: 0.6486\n",
            "Epoch 313/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5947 - acc: 0.6814 - val_loss: 0.6733 - val_acc: 0.6486\n",
            "Epoch 314/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5943 - acc: 0.6814 - val_loss: 0.6730 - val_acc: 0.6486\n",
            "Epoch 315/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5940 - acc: 0.6814 - val_loss: 0.6727 - val_acc: 0.6486\n",
            "Epoch 316/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5937 - acc: 0.6814 - val_loss: 0.6724 - val_acc: 0.6486\n",
            "Epoch 317/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5933 - acc: 0.6814 - val_loss: 0.6721 - val_acc: 0.6486\n",
            "Epoch 318/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5930 - acc: 0.6814 - val_loss: 0.6719 - val_acc: 0.6486\n",
            "Epoch 319/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5927 - acc: 0.6814 - val_loss: 0.6716 - val_acc: 0.6486\n",
            "Epoch 320/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5923 - acc: 0.6814 - val_loss: 0.6713 - val_acc: 0.6486\n",
            "Epoch 321/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5920 - acc: 0.6814 - val_loss: 0.6710 - val_acc: 0.6486\n",
            "Epoch 322/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5917 - acc: 0.6814 - val_loss: 0.6707 - val_acc: 0.6486\n",
            "Epoch 323/500\n",
            "295/295 [==============================] - 0s 35us/step - loss: 0.5913 - acc: 0.6814 - val_loss: 0.6704 - val_acc: 0.6486\n",
            "Epoch 324/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5910 - acc: 0.6814 - val_loss: 0.6701 - val_acc: 0.6486\n",
            "Epoch 325/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5907 - acc: 0.6814 - val_loss: 0.6699 - val_acc: 0.6486\n",
            "Epoch 326/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5904 - acc: 0.6847 - val_loss: 0.6696 - val_acc: 0.6486\n",
            "Epoch 327/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5901 - acc: 0.6847 - val_loss: 0.6693 - val_acc: 0.6486\n",
            "Epoch 328/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5898 - acc: 0.6847 - val_loss: 0.6690 - val_acc: 0.6486\n",
            "Epoch 329/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5894 - acc: 0.6847 - val_loss: 0.6688 - val_acc: 0.6486\n",
            "Epoch 330/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5891 - acc: 0.6847 - val_loss: 0.6685 - val_acc: 0.6486\n",
            "Epoch 331/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5888 - acc: 0.6847 - val_loss: 0.6682 - val_acc: 0.6486\n",
            "Epoch 332/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5885 - acc: 0.6881 - val_loss: 0.6679 - val_acc: 0.6486\n",
            "Epoch 333/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5882 - acc: 0.6847 - val_loss: 0.6677 - val_acc: 0.6486\n",
            "Epoch 334/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5879 - acc: 0.6847 - val_loss: 0.6674 - val_acc: 0.6486\n",
            "Epoch 335/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5876 - acc: 0.6847 - val_loss: 0.6671 - val_acc: 0.6486\n",
            "Epoch 336/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5873 - acc: 0.6847 - val_loss: 0.6668 - val_acc: 0.6486\n",
            "Epoch 337/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5870 - acc: 0.6847 - val_loss: 0.6666 - val_acc: 0.6486\n",
            "Epoch 338/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5868 - acc: 0.6847 - val_loss: 0.6663 - val_acc: 0.6486\n",
            "Epoch 339/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5865 - acc: 0.6847 - val_loss: 0.6660 - val_acc: 0.6622\n",
            "Epoch 340/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5862 - acc: 0.6847 - val_loss: 0.6658 - val_acc: 0.6622\n",
            "Epoch 341/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5859 - acc: 0.6814 - val_loss: 0.6655 - val_acc: 0.6622\n",
            "Epoch 342/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5856 - acc: 0.6814 - val_loss: 0.6652 - val_acc: 0.6622\n",
            "Epoch 343/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5853 - acc: 0.6814 - val_loss: 0.6650 - val_acc: 0.6622\n",
            "Epoch 344/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5851 - acc: 0.6814 - val_loss: 0.6647 - val_acc: 0.6622\n",
            "Epoch 345/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5848 - acc: 0.6814 - val_loss: 0.6644 - val_acc: 0.6622\n",
            "Epoch 346/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5845 - acc: 0.6814 - val_loss: 0.6642 - val_acc: 0.6622\n",
            "Epoch 347/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5842 - acc: 0.6814 - val_loss: 0.6639 - val_acc: 0.6486\n",
            "Epoch 348/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5840 - acc: 0.6814 - val_loss: 0.6637 - val_acc: 0.6486\n",
            "Epoch 349/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5837 - acc: 0.6814 - val_loss: 0.6634 - val_acc: 0.6486\n",
            "Epoch 350/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5834 - acc: 0.6814 - val_loss: 0.6632 - val_acc: 0.6486\n",
            "Epoch 351/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5832 - acc: 0.6814 - val_loss: 0.6629 - val_acc: 0.6486\n",
            "Epoch 352/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5829 - acc: 0.6814 - val_loss: 0.6626 - val_acc: 0.6486\n",
            "Epoch 353/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5826 - acc: 0.6814 - val_loss: 0.6624 - val_acc: 0.6486\n",
            "Epoch 354/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5824 - acc: 0.6814 - val_loss: 0.6621 - val_acc: 0.6486\n",
            "Epoch 355/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5821 - acc: 0.6814 - val_loss: 0.6619 - val_acc: 0.6486\n",
            "Epoch 356/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5819 - acc: 0.6814 - val_loss: 0.6616 - val_acc: 0.6486\n",
            "Epoch 357/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5816 - acc: 0.6814 - val_loss: 0.6614 - val_acc: 0.6486\n",
            "Epoch 358/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5813 - acc: 0.6814 - val_loss: 0.6611 - val_acc: 0.6486\n",
            "Epoch 359/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5811 - acc: 0.6780 - val_loss: 0.6609 - val_acc: 0.6486\n",
            "Epoch 360/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5808 - acc: 0.6780 - val_loss: 0.6606 - val_acc: 0.6486\n",
            "Epoch 361/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5806 - acc: 0.6780 - val_loss: 0.6604 - val_acc: 0.6486\n",
            "Epoch 362/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5804 - acc: 0.6780 - val_loss: 0.6601 - val_acc: 0.6486\n",
            "Epoch 363/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5801 - acc: 0.6780 - val_loss: 0.6599 - val_acc: 0.6486\n",
            "Epoch 364/500\n",
            "295/295 [==============================] - 0s 38us/step - loss: 0.5799 - acc: 0.6780 - val_loss: 0.6596 - val_acc: 0.6486\n",
            "Epoch 365/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5796 - acc: 0.6780 - val_loss: 0.6594 - val_acc: 0.6486\n",
            "Epoch 366/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5794 - acc: 0.6780 - val_loss: 0.6591 - val_acc: 0.6486\n",
            "Epoch 367/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5791 - acc: 0.6780 - val_loss: 0.6589 - val_acc: 0.6486\n",
            "Epoch 368/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5789 - acc: 0.6780 - val_loss: 0.6586 - val_acc: 0.6486\n",
            "Epoch 369/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5787 - acc: 0.6814 - val_loss: 0.6584 - val_acc: 0.6486\n",
            "Epoch 370/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5784 - acc: 0.6814 - val_loss: 0.6582 - val_acc: 0.6486\n",
            "Epoch 371/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5782 - acc: 0.6847 - val_loss: 0.6579 - val_acc: 0.6486\n",
            "Epoch 372/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5780 - acc: 0.6847 - val_loss: 0.6577 - val_acc: 0.6486\n",
            "Epoch 373/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.5777 - acc: 0.6881 - val_loss: 0.6574 - val_acc: 0.6486\n",
            "Epoch 374/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5775 - acc: 0.6881 - val_loss: 0.6572 - val_acc: 0.6486\n",
            "Epoch 375/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5773 - acc: 0.6881 - val_loss: 0.6570 - val_acc: 0.6486\n",
            "Epoch 376/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5771 - acc: 0.6881 - val_loss: 0.6567 - val_acc: 0.6486\n",
            "Epoch 377/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5768 - acc: 0.6881 - val_loss: 0.6565 - val_acc: 0.6486\n",
            "Epoch 378/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5766 - acc: 0.6881 - val_loss: 0.6563 - val_acc: 0.6486\n",
            "Epoch 379/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5764 - acc: 0.6881 - val_loss: 0.6560 - val_acc: 0.6486\n",
            "Epoch 380/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5762 - acc: 0.6881 - val_loss: 0.6558 - val_acc: 0.6486\n",
            "Epoch 381/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5760 - acc: 0.6881 - val_loss: 0.6556 - val_acc: 0.6486\n",
            "Epoch 382/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5757 - acc: 0.6915 - val_loss: 0.6553 - val_acc: 0.6486\n",
            "Epoch 383/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5755 - acc: 0.6915 - val_loss: 0.6551 - val_acc: 0.6486\n",
            "Epoch 384/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5753 - acc: 0.6915 - val_loss: 0.6549 - val_acc: 0.6486\n",
            "Epoch 385/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5751 - acc: 0.6915 - val_loss: 0.6546 - val_acc: 0.6486\n",
            "Epoch 386/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5749 - acc: 0.6915 - val_loss: 0.6544 - val_acc: 0.6486\n",
            "Epoch 387/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5747 - acc: 0.6915 - val_loss: 0.6542 - val_acc: 0.6486\n",
            "Epoch 388/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5745 - acc: 0.6915 - val_loss: 0.6539 - val_acc: 0.6486\n",
            "Epoch 389/500\n",
            "295/295 [==============================] - 0s 59us/step - loss: 0.5743 - acc: 0.6915 - val_loss: 0.6537 - val_acc: 0.6486\n",
            "Epoch 390/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5741 - acc: 0.6915 - val_loss: 0.6535 - val_acc: 0.6486\n",
            "Epoch 391/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5739 - acc: 0.6949 - val_loss: 0.6533 - val_acc: 0.6486\n",
            "Epoch 392/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5737 - acc: 0.7017 - val_loss: 0.6530 - val_acc: 0.6486\n",
            "Epoch 393/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5735 - acc: 0.7017 - val_loss: 0.6528 - val_acc: 0.6486\n",
            "Epoch 394/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5733 - acc: 0.7017 - val_loss: 0.6526 - val_acc: 0.6486\n",
            "Epoch 395/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5731 - acc: 0.7051 - val_loss: 0.6524 - val_acc: 0.6486\n",
            "Epoch 396/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5729 - acc: 0.7051 - val_loss: 0.6521 - val_acc: 0.6486\n",
            "Epoch 397/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5727 - acc: 0.7051 - val_loss: 0.6519 - val_acc: 0.6486\n",
            "Epoch 398/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5725 - acc: 0.7051 - val_loss: 0.6517 - val_acc: 0.6486\n",
            "Epoch 399/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5723 - acc: 0.7051 - val_loss: 0.6515 - val_acc: 0.6486\n",
            "Epoch 400/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5721 - acc: 0.7085 - val_loss: 0.6512 - val_acc: 0.6486\n",
            "Epoch 401/500\n",
            "295/295 [==============================] - 0s 44us/step - loss: 0.5719 - acc: 0.7085 - val_loss: 0.6510 - val_acc: 0.6486\n",
            "Epoch 402/500\n",
            "295/295 [==============================] - 0s 49us/step - loss: 0.5717 - acc: 0.7085 - val_loss: 0.6508 - val_acc: 0.6486\n",
            "Epoch 403/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5715 - acc: 0.7085 - val_loss: 0.6506 - val_acc: 0.6486\n",
            "Epoch 404/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5713 - acc: 0.7119 - val_loss: 0.6504 - val_acc: 0.6486\n",
            "Epoch 405/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5711 - acc: 0.7119 - val_loss: 0.6501 - val_acc: 0.6486\n",
            "Epoch 406/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5709 - acc: 0.7119 - val_loss: 0.6499 - val_acc: 0.6486\n",
            "Epoch 407/500\n",
            "295/295 [==============================] - 0s 33us/step - loss: 0.5707 - acc: 0.7119 - val_loss: 0.6497 - val_acc: 0.6486\n",
            "Epoch 408/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5706 - acc: 0.7119 - val_loss: 0.6495 - val_acc: 0.6486\n",
            "Epoch 409/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5704 - acc: 0.7119 - val_loss: 0.6493 - val_acc: 0.6486\n",
            "Epoch 410/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5702 - acc: 0.7119 - val_loss: 0.6491 - val_acc: 0.6486\n",
            "Epoch 411/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5700 - acc: 0.7119 - val_loss: 0.6489 - val_acc: 0.6486\n",
            "Epoch 412/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5698 - acc: 0.7119 - val_loss: 0.6486 - val_acc: 0.6486\n",
            "Epoch 413/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5697 - acc: 0.7119 - val_loss: 0.6484 - val_acc: 0.6486\n",
            "Epoch 414/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5695 - acc: 0.7119 - val_loss: 0.6482 - val_acc: 0.6486\n",
            "Epoch 415/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5693 - acc: 0.7153 - val_loss: 0.6480 - val_acc: 0.6486\n",
            "Epoch 416/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5691 - acc: 0.7153 - val_loss: 0.6478 - val_acc: 0.6486\n",
            "Epoch 417/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5689 - acc: 0.7153 - val_loss: 0.6476 - val_acc: 0.6486\n",
            "Epoch 418/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5688 - acc: 0.7153 - val_loss: 0.6474 - val_acc: 0.6486\n",
            "Epoch 419/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5686 - acc: 0.7153 - val_loss: 0.6472 - val_acc: 0.6486\n",
            "Epoch 420/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5684 - acc: 0.7153 - val_loss: 0.6470 - val_acc: 0.6486\n",
            "Epoch 421/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5683 - acc: 0.7153 - val_loss: 0.6467 - val_acc: 0.6486\n",
            "Epoch 422/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5681 - acc: 0.7153 - val_loss: 0.6465 - val_acc: 0.6486\n",
            "Epoch 423/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5679 - acc: 0.7153 - val_loss: 0.6463 - val_acc: 0.6486\n",
            "Epoch 424/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5677 - acc: 0.7153 - val_loss: 0.6461 - val_acc: 0.6486\n",
            "Epoch 425/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5676 - acc: 0.7153 - val_loss: 0.6459 - val_acc: 0.6486\n",
            "Epoch 426/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5674 - acc: 0.7153 - val_loss: 0.6457 - val_acc: 0.6486\n",
            "Epoch 427/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5672 - acc: 0.7153 - val_loss: 0.6455 - val_acc: 0.6486\n",
            "Epoch 428/500\n",
            "295/295 [==============================] - 0s 31us/step - loss: 0.5671 - acc: 0.7153 - val_loss: 0.6453 - val_acc: 0.6486\n",
            "Epoch 429/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5669 - acc: 0.7153 - val_loss: 0.6451 - val_acc: 0.6486\n",
            "Epoch 430/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5668 - acc: 0.7153 - val_loss: 0.6449 - val_acc: 0.6486\n",
            "Epoch 431/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5666 - acc: 0.7153 - val_loss: 0.6447 - val_acc: 0.6486\n",
            "Epoch 432/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5664 - acc: 0.7153 - val_loss: 0.6445 - val_acc: 0.6486\n",
            "Epoch 433/500\n",
            "295/295 [==============================] - 0s 14us/step - loss: 0.5663 - acc: 0.7153 - val_loss: 0.6443 - val_acc: 0.6486\n",
            "Epoch 434/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5661 - acc: 0.7153 - val_loss: 0.6441 - val_acc: 0.6486\n",
            "Epoch 435/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5660 - acc: 0.7153 - val_loss: 0.6439 - val_acc: 0.6486\n",
            "Epoch 436/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5658 - acc: 0.7153 - val_loss: 0.6437 - val_acc: 0.6486\n",
            "Epoch 437/500\n",
            "295/295 [==============================] - 0s 24us/step - loss: 0.5656 - acc: 0.7153 - val_loss: 0.6435 - val_acc: 0.6486\n",
            "Epoch 438/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5655 - acc: 0.7153 - val_loss: 0.6433 - val_acc: 0.6486\n",
            "Epoch 439/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5653 - acc: 0.7153 - val_loss: 0.6431 - val_acc: 0.6486\n",
            "Epoch 440/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5652 - acc: 0.7153 - val_loss: 0.6429 - val_acc: 0.6486\n",
            "Epoch 441/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5650 - acc: 0.7153 - val_loss: 0.6427 - val_acc: 0.6486\n",
            "Epoch 442/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5649 - acc: 0.7119 - val_loss: 0.6425 - val_acc: 0.6486\n",
            "Epoch 443/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5647 - acc: 0.7119 - val_loss: 0.6423 - val_acc: 0.6486\n",
            "Epoch 444/500\n",
            "295/295 [==============================] - 0s 27us/step - loss: 0.5646 - acc: 0.7119 - val_loss: 0.6421 - val_acc: 0.6486\n",
            "Epoch 445/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5644 - acc: 0.7119 - val_loss: 0.6419 - val_acc: 0.6486\n",
            "Epoch 446/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5643 - acc: 0.7119 - val_loss: 0.6417 - val_acc: 0.6486\n",
            "Epoch 447/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5641 - acc: 0.7085 - val_loss: 0.6415 - val_acc: 0.6486\n",
            "Epoch 448/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5640 - acc: 0.7085 - val_loss: 0.6413 - val_acc: 0.6486\n",
            "Epoch 449/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5638 - acc: 0.7085 - val_loss: 0.6412 - val_acc: 0.6486\n",
            "Epoch 450/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5637 - acc: 0.7119 - val_loss: 0.6410 - val_acc: 0.6486\n",
            "Epoch 451/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5635 - acc: 0.7085 - val_loss: 0.6408 - val_acc: 0.6486\n",
            "Epoch 452/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5634 - acc: 0.7085 - val_loss: 0.6406 - val_acc: 0.6486\n",
            "Epoch 453/500\n",
            "295/295 [==============================] - 0s 30us/step - loss: 0.5632 - acc: 0.7119 - val_loss: 0.6404 - val_acc: 0.6486\n",
            "Epoch 454/500\n",
            "295/295 [==============================] - 0s 28us/step - loss: 0.5631 - acc: 0.7119 - val_loss: 0.6402 - val_acc: 0.6486\n",
            "Epoch 455/500\n",
            "295/295 [==============================] - 0s 34us/step - loss: 0.5630 - acc: 0.7119 - val_loss: 0.6400 - val_acc: 0.6486\n",
            "Epoch 456/500\n",
            "295/295 [==============================] - 0s 32us/step - loss: 0.5628 - acc: 0.7119 - val_loss: 0.6398 - val_acc: 0.6486\n",
            "Epoch 457/500\n",
            "295/295 [==============================] - 0s 29us/step - loss: 0.5627 - acc: 0.7119 - val_loss: 0.6396 - val_acc: 0.6486\n",
            "Epoch 458/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5625 - acc: 0.7119 - val_loss: 0.6394 - val_acc: 0.6486\n",
            "Epoch 459/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5624 - acc: 0.7119 - val_loss: 0.6393 - val_acc: 0.6486\n",
            "Epoch 460/500\n",
            "295/295 [==============================] - 0s 33us/step - loss: 0.5623 - acc: 0.7119 - val_loss: 0.6391 - val_acc: 0.6486\n",
            "Epoch 461/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5621 - acc: 0.7119 - val_loss: 0.6389 - val_acc: 0.6486\n",
            "Epoch 462/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5620 - acc: 0.7085 - val_loss: 0.6387 - val_acc: 0.6486\n",
            "Epoch 463/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5619 - acc: 0.7085 - val_loss: 0.6385 - val_acc: 0.6486\n",
            "Epoch 464/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5617 - acc: 0.7085 - val_loss: 0.6383 - val_acc: 0.6486\n",
            "Epoch 465/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5616 - acc: 0.7085 - val_loss: 0.6381 - val_acc: 0.6486\n",
            "Epoch 466/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5614 - acc: 0.7085 - val_loss: 0.6380 - val_acc: 0.6486\n",
            "Epoch 467/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5613 - acc: 0.7119 - val_loss: 0.6378 - val_acc: 0.6486\n",
            "Epoch 468/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5612 - acc: 0.7119 - val_loss: 0.6376 - val_acc: 0.6486\n",
            "Epoch 469/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5610 - acc: 0.7119 - val_loss: 0.6374 - val_acc: 0.6486\n",
            "Epoch 470/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5609 - acc: 0.7119 - val_loss: 0.6372 - val_acc: 0.6486\n",
            "Epoch 471/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5608 - acc: 0.7119 - val_loss: 0.6370 - val_acc: 0.6486\n",
            "Epoch 472/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5607 - acc: 0.7119 - val_loss: 0.6369 - val_acc: 0.6486\n",
            "Epoch 473/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5605 - acc: 0.7119 - val_loss: 0.6367 - val_acc: 0.6486\n",
            "Epoch 474/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5604 - acc: 0.7119 - val_loss: 0.6365 - val_acc: 0.6486\n",
            "Epoch 475/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5603 - acc: 0.7119 - val_loss: 0.6363 - val_acc: 0.6486\n",
            "Epoch 476/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5601 - acc: 0.7119 - val_loss: 0.6361 - val_acc: 0.6486\n",
            "Epoch 477/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5600 - acc: 0.7119 - val_loss: 0.6360 - val_acc: 0.6486\n",
            "Epoch 478/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5599 - acc: 0.7119 - val_loss: 0.6358 - val_acc: 0.6486\n",
            "Epoch 479/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5598 - acc: 0.7119 - val_loss: 0.6356 - val_acc: 0.6486\n",
            "Epoch 480/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5596 - acc: 0.7119 - val_loss: 0.6354 - val_acc: 0.6486\n",
            "Epoch 481/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5595 - acc: 0.7119 - val_loss: 0.6353 - val_acc: 0.6486\n",
            "Epoch 482/500\n",
            "295/295 [==============================] - 0s 15us/step - loss: 0.5594 - acc: 0.7119 - val_loss: 0.6351 - val_acc: 0.6486\n",
            "Epoch 483/500\n",
            "295/295 [==============================] - 0s 18us/step - loss: 0.5593 - acc: 0.7119 - val_loss: 0.6349 - val_acc: 0.6486\n",
            "Epoch 484/500\n",
            "295/295 [==============================] - 0s 17us/step - loss: 0.5591 - acc: 0.7119 - val_loss: 0.6347 - val_acc: 0.6486\n",
            "Epoch 485/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5590 - acc: 0.7119 - val_loss: 0.6345 - val_acc: 0.6486\n",
            "Epoch 486/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5589 - acc: 0.7119 - val_loss: 0.6344 - val_acc: 0.6486\n",
            "Epoch 487/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5588 - acc: 0.7119 - val_loss: 0.6342 - val_acc: 0.6486\n",
            "Epoch 488/500\n",
            "295/295 [==============================] - 0s 16us/step - loss: 0.5587 - acc: 0.7119 - val_loss: 0.6340 - val_acc: 0.6486\n",
            "Epoch 489/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5585 - acc: 0.7119 - val_loss: 0.6338 - val_acc: 0.6486\n",
            "Epoch 490/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5584 - acc: 0.7119 - val_loss: 0.6337 - val_acc: 0.6486\n",
            "Epoch 491/500\n",
            "295/295 [==============================] - 0s 23us/step - loss: 0.5583 - acc: 0.7119 - val_loss: 0.6335 - val_acc: 0.6486\n",
            "Epoch 492/500\n",
            "295/295 [==============================] - 0s 25us/step - loss: 0.5582 - acc: 0.7119 - val_loss: 0.6333 - val_acc: 0.6486\n",
            "Epoch 493/500\n",
            "295/295 [==============================] - 0s 26us/step - loss: 0.5581 - acc: 0.7119 - val_loss: 0.6332 - val_acc: 0.6486\n",
            "Epoch 494/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5579 - acc: 0.7119 - val_loss: 0.6330 - val_acc: 0.6486\n",
            "Epoch 495/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5578 - acc: 0.7119 - val_loss: 0.6328 - val_acc: 0.6486\n",
            "Epoch 496/500\n",
            "295/295 [==============================] - 0s 20us/step - loss: 0.5577 - acc: 0.7119 - val_loss: 0.6326 - val_acc: 0.6486\n",
            "Epoch 497/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5576 - acc: 0.7119 - val_loss: 0.6325 - val_acc: 0.6486\n",
            "Epoch 498/500\n",
            "295/295 [==============================] - 0s 19us/step - loss: 0.5575 - acc: 0.7119 - val_loss: 0.6323 - val_acc: 0.6486\n",
            "Epoch 499/500\n",
            "295/295 [==============================] - 0s 21us/step - loss: 0.5574 - acc: 0.7119 - val_loss: 0.6321 - val_acc: 0.6486\n",
            "Epoch 500/500\n",
            "295/295 [==============================] - 0s 22us/step - loss: 0.5573 - acc: 0.7119 - val_loss: 0.6320 - val_acc: 0.6486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCn67LWbWUpf",
        "colab_type": "text"
      },
      "source": [
        "Check out your training set loss and accuracy and your validation set loss and accuracy.  How do they compare?  Does it look like the model has overfit the training data, or are we ok?  (You don't need to write anything, just think about this for a few seconds.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4mSUe7ZQYl",
        "colab_type": "text"
      },
      "source": [
        "# Use the model fit to make predictions.\n",
        "\n",
        "### 2. Calculate test set predictions.\n",
        "\n",
        "As we've seen, once we have an estimated bias $b$ and weights $w$, the activations for a single observation are calculated as\n",
        "\n",
        "\\begin{align*}\n",
        "z^{(i)} &= b + w^T x^{(i)} = b + \\left(x^{(i)}\\right)^T w \\\\\n",
        "a^{(i)} &= \\frac{\\exp\\left(z^{(i)}\\right)}{1 + \\exp\\left(z^{(i)}\\right)}\n",
        "\\end{align*}\n",
        "\n",
        "If we have $m$ observations with corresponding column vectors of features $x^{(1)}, \\ldots, x^{(m)}$, we can organize the first step of this calculation in either of two ways:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} z^{(1)} & z^{(2)} & \\cdots & z^{(m)}\\end{bmatrix} = \\begin{bmatrix} b & b & \\cdots & b \\end{bmatrix} + w^T \\begin{bmatrix} {x^{(1)}} & {x^{(2)}} & \\cdots & {x^{(m)}} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "...or...\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} z^{(1)} \\\\ z^{(2)} \\\\ \\vdots \\\\ z^{(m)}\\end{bmatrix} = \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix} + \\begin{bmatrix} {x^{(1)}}^T \\\\ {x^{(2)}}^T \\\\ \\vdots \\\\ {x^{(m)}}^T \\end{bmatrix} w\n",
        "$$\n",
        "\n",
        "Note that these equations are equivalent; the second is just the transpose of the first.  Andrew Ng's video lectures use the first way of organizing this, and our textbook uses the second.\n",
        "\n",
        "Either way, we then apply the sigmoid activation function elementwise to the vector of $z$'s and predict that a test set observation $y^{(i)} = 1$ if $a^{(i)} \\geq 0.5$.\n",
        "\n",
        "Implement this calculation in the space below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbxrViDOiSZz",
        "colab_type": "code",
        "outputId": "8256c9db-4f56-4e67-af7d-006f65cde2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# This line extracts the estimated weights w and bias b from the model fit\n",
        "(w, b) = logistic_model.layers[0].get_weights()\n",
        "\n",
        "# Calculate the vector z here:\n",
        "# This should involve b, X_test, and w.  Use np.dot() and broadcasting.\n",
        "# Do you need to find any transposes?\n",
        "z = b + np.dot(X_test, w)\n",
        "# OR\n",
        "z = b + np.dot(w.T, X_test.T)\n",
        "\n",
        "# Calculate the activation vector a:\n",
        "a = np.exp(z) / (1 + np.exp(z)) # you'll want to use np.exp()\n",
        "\n",
        "# Find the predicted value y hat, as a logical (lgl) vector\n",
        "y_hat_lgl = (a >= 0.5)\n",
        "\n",
        "# Convert y_hat_logical to a float using the astype function from NumPy.\n",
        "y_hat = y_hat_lgl.astype(float)\n",
        "\n",
        "# For each prediction, determine whether the prediction was correct by\n",
        "# comparing it to the observed test set response.  The result of this\n",
        "# calculation should be a logical vector of the same length as y_test\n",
        "# that is True for cases where the test set prediction was correct and False\n",
        "# for test set cases where the prediction was wrong.\n",
        "y_hat_correct_lgl = (y_hat == y_test)\n",
        "\n",
        "# Determine what proportion of your test set predictions were correct by\n",
        "# calculating the *mean* of the values in the y_hat_correct_lgl variable.\n",
        "# Note that any True values (correct predictions) are converted to 1 and False\n",
        "# values are converted to 0 when you do this calculation.\n",
        "proportion_correct = np.mean(y_hat_correct_lgl)\n",
        "\n",
        "# Print the proportion correct so you can see it after running this code cell\n",
        "print(\"Proportion correct = \" + str(proportion_correct))\n",
        "\n",
        "# The following code does all of the above automagically by calling the keras\n",
        "# function evaluate.  Aren't you glad you know how it works?\n",
        "print(logistic_model.evaluate(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93/93 [==============================] - 0s 95us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5411277663323187, 0.7419354922027999]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9xYPRNOXJgG",
        "colab_type": "text"
      },
      "source": [
        "### 3. If you have extra time, make some exploratory plots of the data.\n",
        "\n",
        "\n",
        "The following code extracts and prints the estimates of $b$ and $w = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$ which you can use to do this calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2L_3J13eWXy",
        "colab_type": "code",
        "outputId": "717f0f0b-e9bf-4754-f1b8-ab9a3fbc1aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "(w, b) = logistic_model.layers[0].get_weights()\n",
        "print(b)\n",
        "print(w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.47226554]\n",
            "[[-0.2382617]\n",
            " [ 0.7055172]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMxw5z4Ue-Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_range = np.arange(start = np.min(X_train[:, 8]), stop = np.max(X_train[:, 8]), step = 1)\n",
        "y_hat = logistic_model.predict(ldl_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHO5DEXhfoDj",
        "colab_type": "code",
        "outputId": "50a7628d-1f70-4ce3-e308-581acba2beb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.scatter(X_train[:, 8], y_train[:, 0], alpha = 0.5)\n",
        "im = ax.plot(x_range, y_hat)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4gE4v8DV4nMwuhyHo4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6f95f7e8-9c3a-45f8-eb49-c9ebadf82fde",
        "id": "4HhbhZGsori3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "# add one line of code here to make a scatter plot\n",
        "scatter = ax.scatter(X_train[:, 2], X_train[:, 7], c = y_train[:, 0], norm = plt.Normalize(-0.2, 1.2), cmap = plt.get_cmap('plasma'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}